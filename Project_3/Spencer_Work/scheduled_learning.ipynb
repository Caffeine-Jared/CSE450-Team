{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Caffeine-Jared/CSE450-Team/blob/main/Initial_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w8AJLFDq_-Er"
      },
      "source": [
        "Initial Model - Working through the example code that was provided by the professor."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KSe9ZbMDBIka"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4ggu-08_3mF",
        "outputId": "6993f428-f72e-4c0b-dec5-0519b65e6523"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 16637 entries, 0 to 16636\n",
            "Data columns (total 12 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   dteday        16637 non-null  object \n",
            " 1   season        16637 non-null  int64  \n",
            " 2   hr            16637 non-null  int64  \n",
            " 3   holiday       16637 non-null  int64  \n",
            " 4   workingday    16637 non-null  int64  \n",
            " 5   weathersit    16637 non-null  int64  \n",
            " 6   hum           16637 non-null  float64\n",
            " 7   windspeed     16637 non-null  int64  \n",
            " 8   temp_c        16637 non-null  float64\n",
            " 9   feels_like_c  16637 non-null  float64\n",
            " 10  casual        16637 non-null  int64  \n",
            " 11  registered    16637 non-null  int64  \n",
            "dtypes: float64(3), int64(8), object(1)\n",
            "memory usage: 1.5+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# loading in the bikes csv\n",
        "bikes_df = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')\n",
        "bikes_mini = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/biking_holdout_test_mini.csv')\n",
        "bikes_holdout = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes_december.csv')\n",
        "\n",
        "# check out the info\n",
        "bikes_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>3.28</td>\n",
              "      <td>3.0014</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0</td>\n",
              "      <td>2.34</td>\n",
              "      <td>1.9982</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0</td>\n",
              "      <td>2.34</td>\n",
              "      <td>1.9982</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0</td>\n",
              "      <td>3.28</td>\n",
              "      <td>3.0014</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0</td>\n",
              "      <td>3.28</td>\n",
              "      <td>3.0014</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dteday  season  hr  holiday  workingday  weathersit   hum  windspeed   \n",
              "0  1/1/11       1   0        0           0           1  0.81          0  \\\n",
              "1  1/1/11       1   1        0           0           1  0.80          0   \n",
              "2  1/1/11       1   2        0           0           1  0.80          0   \n",
              "3  1/1/11       1   3        0           0           1  0.75          0   \n",
              "4  1/1/11       1   4        0           0           1  0.75          0   \n",
              "\n",
              "   temp_c  feels_like_c  casual  registered  \n",
              "0    3.28        3.0014       3          13  \n",
              "1    2.34        1.9982       8          32  \n",
              "2    2.34        1.9982       5          27  \n",
              "3    3.28        3.0014       3          10  \n",
              "4    3.28        3.0014       0           1  "
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bikes_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ifhWb55CmrN",
        "outputId": "35c9566b-913e-4dff-b500-530554f35f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35 entries, 0 to 34\n",
            "Data columns (total 10 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   dteday        35 non-null     object \n",
            " 1   season        35 non-null     int64  \n",
            " 2   hr            35 non-null     int64  \n",
            " 3   holiday       35 non-null     int64  \n",
            " 4   workingday    35 non-null     int64  \n",
            " 5   weathersit    35 non-null     int64  \n",
            " 6   hum           35 non-null     float64\n",
            " 7   windspeed     35 non-null     int64  \n",
            " 8   temp_c        35 non-null     float64\n",
            " 9   feels_like_c  35 non-null     float64\n",
            "dtypes: float64(3), int64(6), object(1)\n",
            "memory usage: 2.9+ KB\n"
          ]
        }
      ],
      "source": [
        "bikes_mini.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.81</td>\n",
              "      <td>9</td>\n",
              "      <td>4.22</td>\n",
              "      <td>1.9982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>6</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.0014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    dteday  season  hr  holiday  workingday  weathersit   hum  windspeed   \n",
              "0  12/1/12       4   0        0           0           1  0.81          0  \\\n",
              "1  12/1/12       4   1        0           0           1  0.81          0   \n",
              "2  12/1/12       4   2        0           0           2  0.81          0   \n",
              "3  12/1/12       4   3        0           0           2  0.81          9   \n",
              "4  12/1/12       4   4        0           0           1  0.81          6   \n",
              "\n",
              "   temp_c  feels_like_c  \n",
              "0    4.22        3.9980  \n",
              "1    4.22        3.9980  \n",
              "2    4.22        3.9980  \n",
              "3    4.22        1.9982  \n",
              "4    4.22        3.0014  "
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bikes_mini.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAVPjqQ0Cqqf",
        "outputId": "c18cf7f5-9b43-4fdf-b1f1-09c793bb0ae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 742 entries, 0 to 741\n",
            "Data columns (total 10 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   dteday        742 non-null    object \n",
            " 1   season        742 non-null    int64  \n",
            " 2   hr            742 non-null    int64  \n",
            " 3   holiday       742 non-null    int64  \n",
            " 4   workingday    742 non-null    int64  \n",
            " 5   weathersit    742 non-null    int64  \n",
            " 6   hum           742 non-null    float64\n",
            " 7   windspeed     742 non-null    int64  \n",
            " 8   temp_c        742 non-null    float64\n",
            " 9   feels_like_c  742 non-null    float64\n",
            "dtypes: float64(3), int64(6), object(1)\n",
            "memory usage: 58.1+ KB\n"
          ]
        }
      ],
      "source": [
        "bikes_holdout.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.81</td>\n",
              "      <td>9</td>\n",
              "      <td>4.22</td>\n",
              "      <td>1.9982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>6</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.0014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    dteday  season  hr  holiday  workingday  weathersit   hum  windspeed   \n",
              "0  12/1/12       4   0        0           0           1  0.81          0  \\\n",
              "1  12/1/12       4   1        0           0           1  0.81          0   \n",
              "2  12/1/12       4   2        0           0           2  0.81          0   \n",
              "3  12/1/12       4   3        0           0           2  0.81          9   \n",
              "4  12/1/12       4   4        0           0           1  0.81          6   \n",
              "\n",
              "   temp_c  feels_like_c  \n",
              "0    4.22        3.9980  \n",
              "1    4.22        3.9980  \n",
              "2    4.22        3.9980  \n",
              "3    4.22        1.9982  \n",
              "4    4.22        3.0014  "
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bikes_holdout.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "VlXM5n8TGNLM"
      },
      "outputs": [],
      "source": [
        "# Preprocessing \n",
        "bikes_df['dteday'] = pd.to_datetime(bikes_df['dteday'], format='%m/%d/%y')\n",
        "bikes_mini['dteday'] = pd.to_datetime(bikes_mini['dteday'], format='%m/%d/%y')\n",
        "bikes_holdout['dteday'] = pd.to_datetime(bikes_holdout['dteday'], format='%m/%d/%y')\n",
        "\n",
        "# create new features - year, month, day, dayofweek\n",
        "bikes_df['year'] = pd.to_datetime(bikes_df['dteday']).dt.year\n",
        "bikes_df['month'] = pd.to_datetime(bikes_df['dteday']).dt.month\n",
        "bikes_df['day'] = pd.to_datetime(bikes_df['dteday']).dt.day\n",
        "bikes_df['dayofweek'] = pd.to_datetime(bikes_df['dteday']).dt.dayofweek\n",
        "\n",
        "bikes_mini['year'] = pd.to_datetime(bikes_mini['dteday']).dt.year\n",
        "bikes_mini['month'] = pd.to_datetime(bikes_mini['dteday']).dt.month\n",
        "bikes_mini['day'] = pd.to_datetime(bikes_mini['dteday']).dt.day\n",
        "bikes_mini['dayofweek'] = pd.to_datetime(bikes_mini['dteday']).dt.dayofweek\n",
        "\n",
        "bikes_holdout['year'] = pd.to_datetime(bikes_holdout['dteday']).dt.year\n",
        "bikes_holdout['month'] = pd.to_datetime(bikes_holdout['dteday']).dt.month\n",
        "bikes_holdout['day'] = pd.to_datetime(bikes_holdout['dteday']).dt.day\n",
        "bikes_holdout['dayofweek'] = pd.to_datetime(bikes_holdout['dteday']).dt.dayofweek\n",
        "\n",
        "# drop dteday column\n",
        "bikes_df = bikes_df.drop('dteday', axis=1)\n",
        "bikes_mini = bikes_mini.drop('dteday', axis=1)\n",
        "bikes_holdout = bikes_holdout.drop('dteday', axis=1)\n",
        "\n",
        "# one hot encoding\n",
        "categorical_features = ['season', 'hr', 'holiday', 'workingday', 'weathersit', 'year', 'month', 'day', 'dayofweek']\n",
        "bikes_df = pd.get_dummies(bikes_df, columns=categorical_features, dtype=int)\n",
        "bikes_mini = pd.get_dummies(bikes_mini, columns=categorical_features, dtype=int)\n",
        "bikes_holdout = pd.get_dummies(bikes_holdout, columns=categorical_features, dtype=int)\n",
        "\n",
        "# min max scaling\n",
        "scaler = MinMaxScaler()\n",
        "bikes_df[['temp_c', 'hum', 'feels_like_c', 'windspeed']] = scaler.fit_transform(bikes_df[['temp_c', 'hum', 'feels_like_c', 'windspeed']])\n",
        "bikes_mini[['temp_c', 'hum', 'feels_like_c', 'windspeed']] = scaler.fit_transform(bikes_mini[['temp_c', 'hum', 'feels_like_c', 'windspeed']])\n",
        "bikes_holdout[['temp_c', 'hum', 'feels_like_c', 'windspeed']] = scaler.fit_transform(bikes_holdout[['temp_c', 'hum', 'feels_like_c', 'windspeed']])\n",
        "\n",
        "# creating a total count column\n",
        "bikes_df['total_count'] = bikes_df['casual'] + bikes_df['registered']\n",
        "# when it comes to adding these columns together, we don't care about the specifics between casual and registered, we just want the total count, as it provides more information\n",
        "# additionally, the questions we need to answer surround total count, not casual or registered\n",
        "# drop casual and registered columns\n",
        "bikes_df = bikes_df.drop(columns=['casual', 'registered'])\n",
        "# it's important to scale the numbers as not scaling would cause the model to think that the total count is more important than the other features\n",
        "# scale bikes_df total_count\n",
        "#bikes_df[['total_count']] = scaler.fit_transform(bikes_df[['total_count']])\n",
        "# features and the target\n",
        "X = bikes_df.drop(columns=['total_count'])\n",
        "y = bikes_df['total_count']\n",
        "\n",
        "# training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "      <th>season_1</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>season_4</th>\n",
              "      <th>hr_0</th>\n",
              "      <th>hr_1</th>\n",
              "      <th>...</th>\n",
              "      <th>day_30</th>\n",
              "      <th>day_31</th>\n",
              "      <th>dayofweek_0</th>\n",
              "      <th>dayofweek_1</th>\n",
              "      <th>dayofweek_2</th>\n",
              "      <th>dayofweek_3</th>\n",
              "      <th>dayofweek_4</th>\n",
              "      <th>dayofweek_5</th>\n",
              "      <th>dayofweek_6</th>\n",
              "      <th>total_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.224490</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.204082</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.204082</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.224490</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.224490</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 93 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    hum  windspeed    temp_c  feels_like_c  season_1  season_2  season_3   \n",
              "0  0.81        0.0  0.224490        0.2879         1         0         0  \\\n",
              "1  0.80        0.0  0.204082        0.2727         1         0         0   \n",
              "2  0.80        0.0  0.204082        0.2727         1         0         0   \n",
              "3  0.75        0.0  0.224490        0.2879         1         0         0   \n",
              "4  0.75        0.0  0.224490        0.2879         1         0         0   \n",
              "\n",
              "   season_4  hr_0  hr_1  ...  day_30  day_31  dayofweek_0  dayofweek_1   \n",
              "0         0     1     0  ...       0       0            0            0  \\\n",
              "1         0     0     1  ...       0       0            0            0   \n",
              "2         0     0     0  ...       0       0            0            0   \n",
              "3         0     0     0  ...       0       0            0            0   \n",
              "4         0     0     0  ...       0       0            0            0   \n",
              "\n",
              "   dayofweek_2  dayofweek_3  dayofweek_4  dayofweek_5  dayofweek_6   \n",
              "0            0            0            0            1            0  \\\n",
              "1            0            0            0            1            0   \n",
              "2            0            0            0            1            0   \n",
              "3            0            0            0            1            0   \n",
              "4            0            0            0            1            0   \n",
              "\n",
              "   total_count  \n",
              "0           16  \n",
              "1           40  \n",
              "2           32  \n",
              "3           13  \n",
              "4            1  \n",
              "\n",
              "[5 rows x 93 columns]"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bikes_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13309, 92)"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "ename": "PermissionError",
          "evalue": "[Errno 13] Permission denied: 'bikes_output.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[79], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m bikes_df\u001b[39m.\u001b[39mhead()\n\u001b[1;32m----> 2\u001b[0m bikes_df\u001b[39m.\u001b[39;49mto_csv(\u001b[39m'\u001b[39;49m\u001b[39mbikes_output.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3761\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3763\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3764\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3765\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3769\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3770\u001b[0m )\n\u001b[1;32m-> 3772\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3773\u001b[0m     path_or_buf,\n\u001b[0;32m   3774\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[0;32m   3775\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3776\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3777\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3778\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3779\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3780\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3781\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3782\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3783\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3784\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3785\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3786\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3787\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3788\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3789\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\formats\\format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1168\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1169\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1185\u001b[0m )\n\u001b[1;32m-> 1186\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1189\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\formats\\csvs.py:240\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    241\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    243\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    244\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    245\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    246\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    247\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    248\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    250\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    251\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    259\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
            "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'bikes_output.csv'"
          ]
        }
      ],
      "source": [
        "bikes_df.head()\n",
        "bikes_df.to_csv('bikes_output.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "      <th>season_4</th>\n",
              "      <th>hr_0</th>\n",
              "      <th>hr_1</th>\n",
              "      <th>hr_2</th>\n",
              "      <th>hr_3</th>\n",
              "      <th>hr_4</th>\n",
              "      <th>...</th>\n",
              "      <th>holiday_0</th>\n",
              "      <th>workingday_0</th>\n",
              "      <th>weathersit_1</th>\n",
              "      <th>weathersit_2</th>\n",
              "      <th>year_2012</th>\n",
              "      <th>month_12</th>\n",
              "      <th>day_1</th>\n",
              "      <th>day_2</th>\n",
              "      <th>dayofweek_5</th>\n",
              "      <th>dayofweek_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.612903</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.363527</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.612903</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.363527</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.612903</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.363527</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.612903</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.181764</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.612903</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.272945</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 39 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        hum  windspeed  temp_c  feels_like_c  season_4  hr_0  hr_1  hr_2   \n",
              "0  0.612903   0.000000   0.125      0.363527         1     1     0     0  \\\n",
              "1  0.612903   0.000000   0.125      0.363527         1     0     1     0   \n",
              "2  0.612903   0.000000   0.125      0.363527         1     0     0     1   \n",
              "3  0.612903   0.529412   0.125      0.181764         1     0     0     0   \n",
              "4  0.612903   0.352941   0.125      0.272945         1     0     0     0   \n",
              "\n",
              "   hr_3  hr_4  ...  holiday_0  workingday_0  weathersit_1  weathersit_2   \n",
              "0     0     0  ...          1             1             1             0  \\\n",
              "1     0     0  ...          1             1             1             0   \n",
              "2     0     0  ...          1             1             0             1   \n",
              "3     1     0  ...          1             1             0             1   \n",
              "4     0     1  ...          1             1             1             0   \n",
              "\n",
              "   year_2012  month_12  day_1  day_2  dayofweek_5  dayofweek_6  \n",
              "0          1         1      1      0            1            0  \n",
              "1          1         1      1      0            1            0  \n",
              "2          1         1      1      0            1            0  \n",
              "3          1         1      1      0            1            0  \n",
              "4          1         1      1      0            1            0  \n",
              "\n",
              "[5 rows x 39 columns]"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bikes_mini.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "      <th>season_1</th>\n",
              "      <th>season_4</th>\n",
              "      <th>hr_0</th>\n",
              "      <th>hr_1</th>\n",
              "      <th>hr_2</th>\n",
              "      <th>hr_3</th>\n",
              "      <th>...</th>\n",
              "      <th>day_29</th>\n",
              "      <th>day_30</th>\n",
              "      <th>day_31</th>\n",
              "      <th>dayofweek_0</th>\n",
              "      <th>dayofweek_1</th>\n",
              "      <th>dayofweek_2</th>\n",
              "      <th>dayofweek_3</th>\n",
              "      <th>dayofweek_4</th>\n",
              "      <th>dayofweek_5</th>\n",
              "      <th>dayofweek_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.724638</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.26087</td>\n",
              "      <td>0.322546</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.724638</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.26087</td>\n",
              "      <td>0.322546</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.724638</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.26087</td>\n",
              "      <td>0.322546</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.724638</td>\n",
              "      <td>0.204545</td>\n",
              "      <td>0.26087</td>\n",
              "      <td>0.258037</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.724638</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.26087</td>\n",
              "      <td>0.290398</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 77 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        hum  windspeed   temp_c  feels_like_c  season_1  season_4  hr_0  hr_1   \n",
              "0  0.724638   0.000000  0.26087      0.322546         0         1     1     0  \\\n",
              "1  0.724638   0.000000  0.26087      0.322546         0         1     0     1   \n",
              "2  0.724638   0.000000  0.26087      0.322546         0         1     0     0   \n",
              "3  0.724638   0.204545  0.26087      0.258037         0         1     0     0   \n",
              "4  0.724638   0.136364  0.26087      0.290398         0         1     0     0   \n",
              "\n",
              "   hr_2  hr_3  ...  day_29  day_30  day_31  dayofweek_0  dayofweek_1   \n",
              "0     0     0  ...       0       0       0            0            0  \\\n",
              "1     0     0  ...       0       0       0            0            0   \n",
              "2     1     0  ...       0       0       0            0            0   \n",
              "3     0     1  ...       0       0       0            0            0   \n",
              "4     0     0  ...       0       0       0            0            0   \n",
              "\n",
              "   dayofweek_2  dayofweek_3  dayofweek_4  dayofweek_5  dayofweek_6  \n",
              "0            0            0            0            1            0  \n",
              "1            0            0            0            1            0  \n",
              "2            0            0            0            1            0  \n",
              "3            0            0            0            1            0  \n",
              "4            0            0            0            1            0  \n",
              "\n",
              "[5 rows x 77 columns]"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bikes_holdout.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 1/300\n",
            "416/416 [==============================] - 1s 981us/step - loss: 7203.2368 - val_loss: 6196.0137 - lr: 0.1000\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 2/300\n",
            "416/416 [==============================] - 0s 803us/step - loss: 3556.0034 - val_loss: 2603.5166 - lr: 0.1000\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 3/300\n",
            "416/416 [==============================] - 0s 812us/step - loss: 2740.4121 - val_loss: 2696.4883 - lr: 0.1000\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 4/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2351.5430 - val_loss: 2174.0667 - lr: 0.1000\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 5/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 2223.9495 - val_loss: 2195.6753 - lr: 0.1000\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 6/300\n",
            "416/416 [==============================] - 0s 824us/step - loss: 1955.2190 - val_loss: 1918.7623 - lr: 0.1000\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 7/300\n",
            "416/416 [==============================] - 0s 766us/step - loss: 1837.6870 - val_loss: 1912.7521 - lr: 0.1000\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 8/300\n",
            "416/416 [==============================] - 0s 776us/step - loss: 1809.1487 - val_loss: 1981.0073 - lr: 0.1000\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 9/300\n",
            "416/416 [==============================] - 0s 735us/step - loss: 1667.7218 - val_loss: 2250.8511 - lr: 0.1000\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 10/300\n",
            "416/416 [==============================] - 0s 762us/step - loss: 1597.0004 - val_loss: 1794.9408 - lr: 0.1000\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 11/300\n",
            "416/416 [==============================] - 0s 804us/step - loss: 1442.8444 - val_loss: 2562.3035 - lr: 0.0900\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 12/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 1473.4987 - val_loss: 1844.0665 - lr: 0.0900\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 13/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 1380.4319 - val_loss: 1731.7083 - lr: 0.0900\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 14/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 1285.4323 - val_loss: 1692.1627 - lr: 0.0900\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 15/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 1276.7582 - val_loss: 1739.1775 - lr: 0.0900\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 16/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 1351.9937 - val_loss: 1649.4900 - lr: 0.0900\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 17/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 1218.9857 - val_loss: 1572.6174 - lr: 0.0900\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 18/300\n",
            "416/416 [==============================] - 0s 774us/step - loss: 1279.8396 - val_loss: 1762.2083 - lr: 0.0900\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 19/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 1173.8386 - val_loss: 1664.0878 - lr: 0.0900\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 20/300\n",
            "416/416 [==============================] - 0s 762us/step - loss: 1140.1338 - val_loss: 2614.3320 - lr: 0.0900\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 21/300\n",
            "416/416 [==============================] - 0s 778us/step - loss: 1168.2264 - val_loss: 1757.0217 - lr: 0.0810\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 22/300\n",
            "416/416 [==============================] - 0s 742us/step - loss: 1114.8489 - val_loss: 1625.9655 - lr: 0.0810\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 23/300\n",
            "416/416 [==============================] - 0s 745us/step - loss: 1091.2118 - val_loss: 1711.6799 - lr: 0.0810\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 24/300\n",
            "416/416 [==============================] - 0s 786us/step - loss: 1140.4863 - val_loss: 1825.1166 - lr: 0.0810\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 25/300\n",
            "416/416 [==============================] - 0s 742us/step - loss: 1034.3044 - val_loss: 1616.8762 - lr: 0.0810\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 26/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 1032.7177 - val_loss: 1642.8674 - lr: 0.0810\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 27/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 1092.1630 - val_loss: 1505.4976 - lr: 0.0810\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 28/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 998.4235 - val_loss: 1631.9611 - lr: 0.0810\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 29/300\n",
            "416/416 [==============================] - 0s 774us/step - loss: 1009.3542 - val_loss: 1660.7988 - lr: 0.0810\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 30/300\n",
            "416/416 [==============================] - 0s 759us/step - loss: 998.2183 - val_loss: 1856.8574 - lr: 0.0810\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 31/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 992.4742 - val_loss: 1608.4565 - lr: 0.0729\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 32/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 957.5157 - val_loss: 1728.7084 - lr: 0.0729\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 33/300\n",
            "416/416 [==============================] - 0s 778us/step - loss: 980.2551 - val_loss: 1559.3320 - lr: 0.0729\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 34/300\n",
            "416/416 [==============================] - 0s 742us/step - loss: 918.9333 - val_loss: 1710.5461 - lr: 0.0729\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 35/300\n",
            "416/416 [==============================] - 0s 815us/step - loss: 955.0179 - val_loss: 1712.5391 - lr: 0.0729\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 36/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 937.4125 - val_loss: 1551.2800 - lr: 0.0729\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 37/300\n",
            "416/416 [==============================] - 0s 750us/step - loss: 919.9768 - val_loss: 1501.5500 - lr: 0.0729\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 38/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 936.3792 - val_loss: 1673.4929 - lr: 0.0729\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 39/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 936.9833 - val_loss: 1585.4976 - lr: 0.0729\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 40/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 916.4084 - val_loss: 1746.6309 - lr: 0.0729\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 41/300\n",
            "416/416 [==============================] - 0s 788us/step - loss: 863.4751 - val_loss: 1640.5818 - lr: 0.0656\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 42/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 908.6700 - val_loss: 1621.7048 - lr: 0.0656\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 43/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 851.9695 - val_loss: 1524.5542 - lr: 0.0656\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 44/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 864.8906 - val_loss: 1664.1665 - lr: 0.0656\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 45/300\n",
            "416/416 [==============================] - 0s 863us/step - loss: 878.5743 - val_loss: 1522.8156 - lr: 0.0656\n",
            "\n",
            "Epoch 46: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 46/300\n",
            "416/416 [==============================] - 0s 788us/step - loss: 888.0488 - val_loss: 1731.7076 - lr: 0.0656\n",
            "\n",
            "Epoch 47: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 47/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 843.0408 - val_loss: 1526.9967 - lr: 0.0656\n",
            "\n",
            "Epoch 48: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 48/300\n",
            "416/416 [==============================] - 0s 745us/step - loss: 820.8212 - val_loss: 1620.8440 - lr: 0.0656\n",
            "\n",
            "Epoch 49: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 49/300\n",
            "416/416 [==============================] - 0s 812us/step - loss: 847.5452 - val_loss: 1554.8462 - lr: 0.0656\n",
            "\n",
            "Epoch 50: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 50/300\n",
            "416/416 [==============================] - 0s 774us/step - loss: 855.3009 - val_loss: 1994.0288 - lr: 0.0656\n",
            "\n",
            "Epoch 51: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 51/300\n",
            "416/416 [==============================] - 0s 783us/step - loss: 799.1447 - val_loss: 1569.6945 - lr: 0.0590\n",
            "\n",
            "Epoch 52: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 52/300\n",
            "416/416 [==============================] - 0s 815us/step - loss: 794.4487 - val_loss: 1694.4960 - lr: 0.0590\n",
            "\n",
            "Epoch 53: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 53/300\n",
            "416/416 [==============================] - 0s 793us/step - loss: 814.2671 - val_loss: 1513.6599 - lr: 0.0590\n",
            "\n",
            "Epoch 54: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 54/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 787.5611 - val_loss: 1596.8839 - lr: 0.0590\n",
            "\n",
            "Epoch 55: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 55/300\n",
            "416/416 [==============================] - 0s 762us/step - loss: 820.7181 - val_loss: 1613.4518 - lr: 0.0590\n",
            "\n",
            "Epoch 56: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 56/300\n",
            "416/416 [==============================] - 0s 740us/step - loss: 812.7237 - val_loss: 2041.6179 - lr: 0.0590\n",
            "\n",
            "Epoch 57: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 57/300\n",
            "416/416 [==============================] - 0s 764us/step - loss: 816.5314 - val_loss: 1808.1812 - lr: 0.0590\n",
            "\n",
            "Epoch 58: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 58/300\n",
            "416/416 [==============================] - 0s 771us/step - loss: 800.1920 - val_loss: 1768.3771 - lr: 0.0590\n",
            "\n",
            "Epoch 59: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 59/300\n",
            "416/416 [==============================] - 0s 754us/step - loss: 804.0242 - val_loss: 1561.7902 - lr: 0.0590\n",
            "\n",
            "Epoch 60: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 60/300\n",
            "416/416 [==============================] - 0s 721us/step - loss: 757.3280 - val_loss: 1592.5978 - lr: 0.0590\n",
            "\n",
            "Epoch 61: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 61/300\n",
            "416/416 [==============================] - 0s 769us/step - loss: 781.0310 - val_loss: 1530.5039 - lr: 0.0531\n",
            "\n",
            "Epoch 62: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 62/300\n",
            "416/416 [==============================] - 0s 750us/step - loss: 748.3268 - val_loss: 1678.1367 - lr: 0.0531\n",
            "\n",
            "Epoch 63: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 63/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 771.2498 - val_loss: 1687.6576 - lr: 0.0531\n",
            "\n",
            "Epoch 64: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 64/300\n",
            "416/416 [==============================] - 0s 745us/step - loss: 754.8282 - val_loss: 1657.4719 - lr: 0.0531\n",
            "\n",
            "Epoch 65: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 65/300\n",
            "416/416 [==============================] - 0s 766us/step - loss: 736.2488 - val_loss: 1634.9581 - lr: 0.0531\n",
            "\n",
            "Epoch 66: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 66/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 763.0616 - val_loss: 1519.9595 - lr: 0.0531\n",
            "\n",
            "Epoch 67: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 67/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 763.2228 - val_loss: 1563.3130 - lr: 0.0531\n",
            "\n",
            "Epoch 68: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 68/300\n",
            "416/416 [==============================] - 0s 762us/step - loss: 753.1395 - val_loss: 1607.2018 - lr: 0.0531\n",
            "\n",
            "Epoch 69: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 69/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 759.7682 - val_loss: 1716.9548 - lr: 0.0531\n",
            "\n",
            "Epoch 70: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 70/300\n",
            "416/416 [==============================] - 0s 762us/step - loss: 763.0668 - val_loss: 1564.4003 - lr: 0.0531\n",
            "\n",
            "Epoch 71: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 71/300\n",
            "416/416 [==============================] - 0s 776us/step - loss: 715.5500 - val_loss: 1602.7942 - lr: 0.0478\n",
            "\n",
            "Epoch 72: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 72/300\n",
            "416/416 [==============================] - 0s 764us/step - loss: 748.7035 - val_loss: 1636.1426 - lr: 0.0478\n",
            "\n",
            "Epoch 73: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 73/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 729.9684 - val_loss: 1630.1177 - lr: 0.0478\n",
            "\n",
            "Epoch 74: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 74/300\n",
            "416/416 [==============================] - 0s 788us/step - loss: 744.1986 - val_loss: 1861.0724 - lr: 0.0478\n",
            "\n",
            "Epoch 75: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 75/300\n",
            "416/416 [==============================] - 0s 759us/step - loss: 718.7775 - val_loss: 1550.4622 - lr: 0.0478\n",
            "\n",
            "Epoch 76: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 76/300\n",
            "416/416 [==============================] - 0s 774us/step - loss: 718.9602 - val_loss: 1551.2863 - lr: 0.0478\n",
            "\n",
            "Epoch 77: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 77/300\n",
            "416/416 [==============================] - 0s 735us/step - loss: 731.7051 - val_loss: 1792.5510 - lr: 0.0478\n",
            "\n",
            "Epoch 78: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 78/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 701.4203 - val_loss: 1525.6659 - lr: 0.0478\n",
            "\n",
            "Epoch 79: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 79/300\n",
            "416/416 [==============================] - 0s 786us/step - loss: 709.3655 - val_loss: 1667.6165 - lr: 0.0478\n",
            "\n",
            "Epoch 80: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 80/300\n",
            "416/416 [==============================] - 0s 728us/step - loss: 704.0950 - val_loss: 1566.1556 - lr: 0.0478\n",
            "\n",
            "Epoch 81: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 81/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 701.3607 - val_loss: 1656.1196 - lr: 0.0430\n",
            "\n",
            "Epoch 82: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 82/300\n",
            "416/416 [==============================] - 0s 783us/step - loss: 685.1766 - val_loss: 1577.7852 - lr: 0.0430\n",
            "\n",
            "Epoch 83: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 83/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 699.9335 - val_loss: 1566.9192 - lr: 0.0430\n",
            "\n",
            "Epoch 84: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 84/300\n",
            "416/416 [==============================] - 0s 754us/step - loss: 696.5473 - val_loss: 1607.0873 - lr: 0.0430\n",
            "\n",
            "Epoch 85: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 85/300\n",
            "416/416 [==============================] - 0s 740us/step - loss: 694.4672 - val_loss: 1575.2812 - lr: 0.0430\n",
            "\n",
            "Epoch 86: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 86/300\n",
            "416/416 [==============================] - 0s 735us/step - loss: 677.9188 - val_loss: 1589.0388 - lr: 0.0430\n",
            "\n",
            "Epoch 87: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 87/300\n",
            "416/416 [==============================] - 0s 740us/step - loss: 691.1226 - val_loss: 1606.9545 - lr: 0.0430\n",
            "\n",
            "Epoch 88: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 88/300\n",
            "416/416 [==============================] - 0s 735us/step - loss: 690.4712 - val_loss: 1582.0892 - lr: 0.0430\n",
            "\n",
            "Epoch 89: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 89/300\n",
            "416/416 [==============================] - 0s 786us/step - loss: 672.5812 - val_loss: 1727.7278 - lr: 0.0430\n",
            "\n",
            "Epoch 90: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 90/300\n",
            "416/416 [==============================] - 0s 766us/step - loss: 696.5659 - val_loss: 1597.7240 - lr: 0.0430\n",
            "\n",
            "Epoch 91: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 91/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 664.1739 - val_loss: 1567.5382 - lr: 0.0387\n",
            "\n",
            "Epoch 92: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 92/300\n",
            "416/416 [==============================] - 0s 757us/step - loss: 664.9793 - val_loss: 1562.9839 - lr: 0.0387\n",
            "\n",
            "Epoch 93: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 93/300\n",
            "416/416 [==============================] - 0s 769us/step - loss: 673.1193 - val_loss: 1602.7709 - lr: 0.0387\n",
            "\n",
            "Epoch 94: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 94/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 664.5703 - val_loss: 1617.9386 - lr: 0.0387\n",
            "\n",
            "Epoch 95: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 95/300\n",
            "416/416 [==============================] - 0s 791us/step - loss: 663.2626 - val_loss: 1631.0779 - lr: 0.0387\n",
            "\n",
            "Epoch 96: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 96/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 662.7847 - val_loss: 1586.1731 - lr: 0.0387\n",
            "\n",
            "Epoch 97: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 97/300\n",
            "416/416 [==============================] - 0s 786us/step - loss: 689.2128 - val_loss: 1617.5597 - lr: 0.0387\n",
            "\n",
            "Epoch 98: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 98/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 708.5681 - val_loss: 1783.4218 - lr: 0.0387\n",
            "\n",
            "Epoch 99: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 99/300\n",
            "416/416 [==============================] - 0s 742us/step - loss: 676.5566 - val_loss: 1583.1926 - lr: 0.0387\n",
            "\n",
            "Epoch 100: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 100/300\n",
            "416/416 [==============================] - 0s 776us/step - loss: 680.0354 - val_loss: 1588.5466 - lr: 0.0387\n",
            "\n",
            "Epoch 101: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 101/300\n",
            "416/416 [==============================] - 0s 815us/step - loss: 652.6669 - val_loss: 1547.4624 - lr: 0.0349\n",
            "\n",
            "Epoch 102: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 102/300\n",
            "416/416 [==============================] - 0s 776us/step - loss: 650.2348 - val_loss: 1581.9891 - lr: 0.0349\n",
            "\n",
            "Epoch 103: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 103/300\n",
            "416/416 [==============================] - 0s 781us/step - loss: 647.8281 - val_loss: 1565.9622 - lr: 0.0349\n",
            "\n",
            "Epoch 104: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 104/300\n",
            "416/416 [==============================] - 0s 786us/step - loss: 640.2076 - val_loss: 1592.7888 - lr: 0.0349\n",
            "\n",
            "Epoch 105: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 105/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 652.6438 - val_loss: 1566.9011 - lr: 0.0349\n",
            "\n",
            "Epoch 106: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 106/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 634.1393 - val_loss: 1647.0701 - lr: 0.0349\n",
            "\n",
            "Epoch 107: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 107/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 647.2118 - val_loss: 1580.7540 - lr: 0.0349\n",
            "\n",
            "Epoch 108: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 108/300\n",
            "416/416 [==============================] - 0s 819us/step - loss: 661.6256 - val_loss: 1584.1945 - lr: 0.0349\n",
            "\n",
            "Epoch 109: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 109/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 645.9909 - val_loss: 1545.5583 - lr: 0.0349\n",
            "\n",
            "Epoch 110: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 110/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 648.1971 - val_loss: 1638.6085 - lr: 0.0349\n",
            "\n",
            "Epoch 111: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 111/300\n",
            "416/416 [==============================] - 0s 869us/step - loss: 648.6962 - val_loss: 1575.2305 - lr: 0.0314\n",
            "\n",
            "Epoch 112: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 112/300\n",
            "416/416 [==============================] - 0s 849us/step - loss: 635.1045 - val_loss: 1578.0956 - lr: 0.0314\n",
            "\n",
            "Epoch 113: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 113/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 617.2965 - val_loss: 1565.8711 - lr: 0.0314\n",
            "\n",
            "Epoch 114: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 114/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 636.3144 - val_loss: 1622.3497 - lr: 0.0314\n",
            "\n",
            "Epoch 115: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 115/300\n",
            "416/416 [==============================] - 0s 911us/step - loss: 624.8359 - val_loss: 1586.5291 - lr: 0.0314\n",
            "\n",
            "Epoch 116: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 116/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 621.8587 - val_loss: 1674.2147 - lr: 0.0314\n",
            "\n",
            "Epoch 117: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 117/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 631.4573 - val_loss: 1624.2626 - lr: 0.0314\n",
            "\n",
            "Epoch 118: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 118/300\n",
            "416/416 [==============================] - 0s 824us/step - loss: 627.5250 - val_loss: 1564.9348 - lr: 0.0314\n",
            "\n",
            "Epoch 119: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 119/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 620.8550 - val_loss: 1656.8405 - lr: 0.0314\n",
            "\n",
            "Epoch 120: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 120/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 629.2806 - val_loss: 1607.6608 - lr: 0.0314\n",
            "\n",
            "Epoch 121: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 121/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 613.9108 - val_loss: 1574.9464 - lr: 0.0282\n",
            "\n",
            "Epoch 122: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 122/300\n",
            "416/416 [==============================] - 0s 825us/step - loss: 610.1830 - val_loss: 1567.4139 - lr: 0.0282\n",
            "\n",
            "Epoch 123: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 123/300\n",
            "416/416 [==============================] - 0s 813us/step - loss: 614.9531 - val_loss: 1658.0499 - lr: 0.0282\n",
            "\n",
            "Epoch 124: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 124/300\n",
            "416/416 [==============================] - 0s 799us/step - loss: 621.1019 - val_loss: 1561.8279 - lr: 0.0282\n",
            "\n",
            "Epoch 125: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 125/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 621.9618 - val_loss: 1604.6320 - lr: 0.0282\n",
            "\n",
            "Epoch 126: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 126/300\n",
            "416/416 [==============================] - 0s 814us/step - loss: 622.5012 - val_loss: 1581.6473 - lr: 0.0282\n",
            "\n",
            "Epoch 127: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 127/300\n",
            "416/416 [==============================] - 0s 743us/step - loss: 606.0710 - val_loss: 1572.6472 - lr: 0.0282\n",
            "\n",
            "Epoch 128: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 128/300\n",
            "416/416 [==============================] - 0s 815us/step - loss: 612.8260 - val_loss: 1605.9308 - lr: 0.0282\n",
            "\n",
            "Epoch 129: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 129/300\n",
            "416/416 [==============================] - 0s 769us/step - loss: 619.0001 - val_loss: 1581.6954 - lr: 0.0282\n",
            "\n",
            "Epoch 130: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 130/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 604.5667 - val_loss: 1593.4032 - lr: 0.0282\n",
            "\n",
            "Epoch 131: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 131/300\n",
            "416/416 [==============================] - 0s 808us/step - loss: 592.9718 - val_loss: 1585.1400 - lr: 0.0254\n",
            "\n",
            "Epoch 132: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 132/300\n",
            "416/416 [==============================] - 0s 778us/step - loss: 610.6180 - val_loss: 1570.7384 - lr: 0.0254\n",
            "\n",
            "Epoch 133: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 133/300\n",
            "416/416 [==============================] - 0s 763us/step - loss: 607.4409 - val_loss: 1575.0315 - lr: 0.0254\n",
            "\n",
            "Epoch 134: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 134/300\n",
            "416/416 [==============================] - 0s 785us/step - loss: 600.9131 - val_loss: 1668.0627 - lr: 0.0254\n",
            "\n",
            "Epoch 135: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 135/300\n",
            "416/416 [==============================] - 0s 845us/step - loss: 598.8799 - val_loss: 1620.2480 - lr: 0.0254\n",
            "\n",
            "Epoch 136: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 136/300\n",
            "416/416 [==============================] - 0s 815us/step - loss: 596.3650 - val_loss: 1583.0237 - lr: 0.0254\n",
            "\n",
            "Epoch 137: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 137/300\n",
            "416/416 [==============================] - 0s 770us/step - loss: 584.2625 - val_loss: 1686.4076 - lr: 0.0254\n",
            "\n",
            "Epoch 138: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 138/300\n",
            "416/416 [==============================] - 0s 766us/step - loss: 599.4421 - val_loss: 1573.0604 - lr: 0.0254\n",
            "\n",
            "Epoch 139: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 139/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 601.3950 - val_loss: 1601.2208 - lr: 0.0254\n",
            "\n",
            "Epoch 140: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 140/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 595.6750 - val_loss: 1590.5591 - lr: 0.0254\n",
            "\n",
            "Epoch 141: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 141/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 597.9691 - val_loss: 1665.0978 - lr: 0.0229\n",
            "\n",
            "Epoch 142: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 142/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 590.8538 - val_loss: 1559.7434 - lr: 0.0229\n",
            "\n",
            "Epoch 143: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 143/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 586.4434 - val_loss: 1575.1241 - lr: 0.0229\n",
            "\n",
            "Epoch 144: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 144/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 584.1355 - val_loss: 1583.7771 - lr: 0.0229\n",
            "\n",
            "Epoch 145: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 145/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 593.1003 - val_loss: 1778.4408 - lr: 0.0229\n",
            "\n",
            "Epoch 146: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 146/300\n",
            "416/416 [==============================] - 0s 766us/step - loss: 596.7570 - val_loss: 1569.4647 - lr: 0.0229\n",
            "\n",
            "Epoch 147: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 147/300\n",
            "416/416 [==============================] - 0s 764us/step - loss: 586.2036 - val_loss: 1694.2330 - lr: 0.0229\n",
            "\n",
            "Epoch 148: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 148/300\n",
            "416/416 [==============================] - 0s 766us/step - loss: 584.4091 - val_loss: 1598.2247 - lr: 0.0229\n",
            "\n",
            "Epoch 149: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 149/300\n",
            "416/416 [==============================] - 0s 752us/step - loss: 582.6305 - val_loss: 1581.3025 - lr: 0.0229\n",
            "\n",
            "Epoch 150: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 150/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 579.3621 - val_loss: 1585.4207 - lr: 0.0229\n",
            "\n",
            "Epoch 151: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 151/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 582.2943 - val_loss: 1566.6194 - lr: 0.0206\n",
            "\n",
            "Epoch 152: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 152/300\n",
            "416/416 [==============================] - 0s 824us/step - loss: 579.6196 - val_loss: 1620.0197 - lr: 0.0206\n",
            "\n",
            "Epoch 153: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 153/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 584.5786 - val_loss: 1563.9373 - lr: 0.0206\n",
            "\n",
            "Epoch 154: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 154/300\n",
            "416/416 [==============================] - 0s 735us/step - loss: 580.1354 - val_loss: 1566.8256 - lr: 0.0206\n",
            "\n",
            "Epoch 155: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 155/300\n",
            "416/416 [==============================] - 0s 762us/step - loss: 572.4965 - val_loss: 1586.1693 - lr: 0.0206\n",
            "\n",
            "Epoch 156: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 156/300\n",
            "416/416 [==============================] - 0s 740us/step - loss: 579.7440 - val_loss: 1569.1583 - lr: 0.0206\n",
            "\n",
            "Epoch 157: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 157/300\n",
            "416/416 [==============================] - 0s 781us/step - loss: 578.1758 - val_loss: 1545.1608 - lr: 0.0206\n",
            "\n",
            "Epoch 158: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 158/300\n",
            "416/416 [==============================] - 0s 764us/step - loss: 572.4801 - val_loss: 1641.3640 - lr: 0.0206\n",
            "\n",
            "Epoch 159: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 159/300\n",
            "416/416 [==============================] - 0s 766us/step - loss: 579.0715 - val_loss: 1561.5431 - lr: 0.0206\n",
            "\n",
            "Epoch 160: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 160/300\n",
            "416/416 [==============================] - 0s 747us/step - loss: 586.6943 - val_loss: 1580.3813 - lr: 0.0206\n",
            "\n",
            "Epoch 161: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 161/300\n",
            "416/416 [==============================] - 0s 776us/step - loss: 569.8093 - val_loss: 1589.2375 - lr: 0.0185\n",
            "\n",
            "Epoch 162: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 162/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 576.8221 - val_loss: 1600.6614 - lr: 0.0185\n",
            "\n",
            "Epoch 163: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 163/300\n",
            "416/416 [==============================] - 0s 781us/step - loss: 566.6225 - val_loss: 1553.6974 - lr: 0.0185\n",
            "\n",
            "Epoch 164: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 164/300\n",
            "416/416 [==============================] - 0s 781us/step - loss: 568.4888 - val_loss: 1581.9070 - lr: 0.0185\n",
            "\n",
            "Epoch 165: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 165/300\n",
            "416/416 [==============================] - 0s 778us/step - loss: 576.4888 - val_loss: 1572.8110 - lr: 0.0185\n",
            "\n",
            "Epoch 166: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 166/300\n",
            "416/416 [==============================] - 0s 752us/step - loss: 569.5193 - val_loss: 1606.0150 - lr: 0.0185\n",
            "\n",
            "Epoch 167: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 167/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 569.8679 - val_loss: 1574.1952 - lr: 0.0185\n",
            "\n",
            "Epoch 168: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 168/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 572.3372 - val_loss: 1577.5957 - lr: 0.0185\n",
            "\n",
            "Epoch 169: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 169/300\n",
            "416/416 [==============================] - 0s 745us/step - loss: 565.8981 - val_loss: 1562.0120 - lr: 0.0185\n",
            "\n",
            "Epoch 170: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 170/300\n",
            "416/416 [==============================] - 0s 754us/step - loss: 566.3295 - val_loss: 1583.5282 - lr: 0.0185\n",
            "\n",
            "Epoch 171: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 171/300\n",
            "416/416 [==============================] - 0s 769us/step - loss: 557.4678 - val_loss: 1555.8612 - lr: 0.0167\n",
            "\n",
            "Epoch 172: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 172/300\n",
            "416/416 [==============================] - 0s 774us/step - loss: 561.5794 - val_loss: 1578.2201 - lr: 0.0167\n",
            "\n",
            "Epoch 173: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 173/300\n",
            "416/416 [==============================] - 0s 728us/step - loss: 562.2018 - val_loss: 1599.6581 - lr: 0.0167\n",
            "\n",
            "Epoch 174: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 174/300\n",
            "416/416 [==============================] - 0s 766us/step - loss: 560.4583 - val_loss: 1628.0031 - lr: 0.0167\n",
            "\n",
            "Epoch 175: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 175/300\n",
            "416/416 [==============================] - 0s 757us/step - loss: 559.4032 - val_loss: 1555.1315 - lr: 0.0167\n",
            "\n",
            "Epoch 176: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 176/300\n",
            "416/416 [==============================] - 0s 764us/step - loss: 558.2010 - val_loss: 1578.9487 - lr: 0.0167\n",
            "\n",
            "Epoch 177: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 177/300\n",
            "416/416 [==============================] - 0s 752us/step - loss: 563.9871 - val_loss: 1566.9297 - lr: 0.0167\n",
            "\n",
            "Epoch 178: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 178/300\n",
            "416/416 [==============================] - 0s 757us/step - loss: 561.3397 - val_loss: 1571.4713 - lr: 0.0167\n",
            "\n",
            "Epoch 179: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 179/300\n",
            "416/416 [==============================] - 0s 747us/step - loss: 562.6779 - val_loss: 1584.8151 - lr: 0.0167\n",
            "\n",
            "Epoch 180: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 180/300\n",
            "416/416 [==============================] - 0s 783us/step - loss: 558.6222 - val_loss: 1683.4657 - lr: 0.0167\n",
            "\n",
            "Epoch 181: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 181/300\n",
            "416/416 [==============================] - 0s 815us/step - loss: 554.4736 - val_loss: 1549.1853 - lr: 0.0150\n",
            "\n",
            "Epoch 182: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 182/300\n",
            "416/416 [==============================] - 0s 788us/step - loss: 549.5978 - val_loss: 1594.8813 - lr: 0.0150\n",
            "\n",
            "Epoch 183: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 183/300\n",
            "416/416 [==============================] - 0s 742us/step - loss: 554.0616 - val_loss: 1575.1884 - lr: 0.0150\n",
            "\n",
            "Epoch 184: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 184/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 553.8261 - val_loss: 1562.8655 - lr: 0.0150\n",
            "\n",
            "Epoch 185: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 185/300\n",
            "416/416 [==============================] - 0s 793us/step - loss: 552.4070 - val_loss: 1571.9467 - lr: 0.0150\n",
            "\n",
            "Epoch 186: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 186/300\n",
            "416/416 [==============================] - 0s 745us/step - loss: 558.8684 - val_loss: 1576.6747 - lr: 0.0150\n",
            "\n",
            "Epoch 187: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 187/300\n",
            "416/416 [==============================] - 0s 735us/step - loss: 549.2984 - val_loss: 1551.3431 - lr: 0.0150\n",
            "\n",
            "Epoch 188: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 188/300\n",
            "416/416 [==============================] - 0s 781us/step - loss: 551.6468 - val_loss: 1598.7648 - lr: 0.0150\n",
            "\n",
            "Epoch 189: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 189/300\n",
            "416/416 [==============================] - 0s 747us/step - loss: 561.3721 - val_loss: 1646.6842 - lr: 0.0150\n",
            "\n",
            "Epoch 190: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 190/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 550.7914 - val_loss: 1542.9845 - lr: 0.0150\n",
            "\n",
            "Epoch 191: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 191/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 549.8379 - val_loss: 1576.5936 - lr: 0.0135\n",
            "\n",
            "Epoch 192: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 192/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 546.5358 - val_loss: 1575.4983 - lr: 0.0135\n",
            "\n",
            "Epoch 193: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 193/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 547.7551 - val_loss: 1571.4940 - lr: 0.0135\n",
            "\n",
            "Epoch 194: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 194/300\n",
            "416/416 [==============================] - 0s 819us/step - loss: 546.9462 - val_loss: 1565.4869 - lr: 0.0135\n",
            "\n",
            "Epoch 195: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 195/300\n",
            "416/416 [==============================] - 0s 742us/step - loss: 544.1792 - val_loss: 1657.1145 - lr: 0.0135\n",
            "\n",
            "Epoch 196: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 196/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 553.9267 - val_loss: 1597.9921 - lr: 0.0135\n",
            "\n",
            "Epoch 197: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 197/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 549.5266 - val_loss: 1580.7990 - lr: 0.0135\n",
            "\n",
            "Epoch 198: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 198/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 548.3477 - val_loss: 1563.6525 - lr: 0.0135\n",
            "\n",
            "Epoch 199: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 199/300\n",
            "416/416 [==============================] - 0s 778us/step - loss: 547.0837 - val_loss: 1577.3973 - lr: 0.0135\n",
            "\n",
            "Epoch 200: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 200/300\n",
            "416/416 [==============================] - 0s 786us/step - loss: 548.8901 - val_loss: 1649.1609 - lr: 0.0135\n",
            "\n",
            "Epoch 201: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 201/300\n",
            "416/416 [==============================] - 0s 757us/step - loss: 540.8861 - val_loss: 1581.6687 - lr: 0.0122\n",
            "\n",
            "Epoch 202: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 202/300\n",
            "416/416 [==============================] - 0s 781us/step - loss: 550.0347 - val_loss: 1573.3889 - lr: 0.0122\n",
            "\n",
            "Epoch 203: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 203/300\n",
            "416/416 [==============================] - 0s 766us/step - loss: 538.7321 - val_loss: 1593.6884 - lr: 0.0122\n",
            "\n",
            "Epoch 204: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 204/300\n",
            "416/416 [==============================] - 0s 819us/step - loss: 540.9138 - val_loss: 1564.5389 - lr: 0.0122\n",
            "\n",
            "Epoch 205: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 205/300\n",
            "416/416 [==============================] - 0s 812us/step - loss: 541.6743 - val_loss: 1575.8627 - lr: 0.0122\n",
            "\n",
            "Epoch 206: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 206/300\n",
            "416/416 [==============================] - 0s 757us/step - loss: 544.3647 - val_loss: 1576.1440 - lr: 0.0122\n",
            "\n",
            "Epoch 207: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 207/300\n",
            "416/416 [==============================] - 0s 783us/step - loss: 541.4295 - val_loss: 1601.7970 - lr: 0.0122\n",
            "\n",
            "Epoch 208: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 208/300\n",
            "416/416 [==============================] - 0s 742us/step - loss: 537.5718 - val_loss: 1597.5841 - lr: 0.0122\n",
            "\n",
            "Epoch 209: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 209/300\n",
            "416/416 [==============================] - 0s 786us/step - loss: 543.8964 - val_loss: 1592.9004 - lr: 0.0122\n",
            "\n",
            "Epoch 210: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 210/300\n",
            "416/416 [==============================] - 0s 759us/step - loss: 547.3688 - val_loss: 1553.6038 - lr: 0.0122\n",
            "\n",
            "Epoch 211: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 211/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 536.1794 - val_loss: 1607.2208 - lr: 0.0109\n",
            "\n",
            "Epoch 212: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 212/300\n",
            "416/416 [==============================] - 0s 728us/step - loss: 537.9745 - val_loss: 1574.9545 - lr: 0.0109\n",
            "\n",
            "Epoch 213: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 213/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 538.2929 - val_loss: 1599.4091 - lr: 0.0109\n",
            "\n",
            "Epoch 214: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 214/300\n",
            "416/416 [==============================] - 0s 750us/step - loss: 535.3949 - val_loss: 1576.1874 - lr: 0.0109\n",
            "\n",
            "Epoch 215: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 215/300\n",
            "416/416 [==============================] - 0s 742us/step - loss: 534.3771 - val_loss: 1610.5236 - lr: 0.0109\n",
            "\n",
            "Epoch 216: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 216/300\n",
            "416/416 [==============================] - 0s 745us/step - loss: 540.1909 - val_loss: 1563.9579 - lr: 0.0109\n",
            "\n",
            "Epoch 217: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 217/300\n",
            "416/416 [==============================] - 0s 764us/step - loss: 532.4623 - val_loss: 1584.7273 - lr: 0.0109\n",
            "\n",
            "Epoch 218: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 218/300\n",
            "416/416 [==============================] - 0s 733us/step - loss: 535.9324 - val_loss: 1572.4110 - lr: 0.0109\n",
            "\n",
            "Epoch 219: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 219/300\n",
            "416/416 [==============================] - 0s 735us/step - loss: 536.8836 - val_loss: 1566.1890 - lr: 0.0109\n",
            "\n",
            "Epoch 220: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 220/300\n",
            "416/416 [==============================] - 0s 747us/step - loss: 539.1820 - val_loss: 1578.9738 - lr: 0.0109\n",
            "\n",
            "Epoch 221: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 221/300\n",
            "416/416 [==============================] - 0s 725us/step - loss: 529.0052 - val_loss: 1634.0265 - lr: 0.0098\n",
            "\n",
            "Epoch 222: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 222/300\n",
            "416/416 [==============================] - 0s 757us/step - loss: 529.0019 - val_loss: 1570.0861 - lr: 0.0098\n",
            "\n",
            "Epoch 223: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 223/300\n",
            "416/416 [==============================] - 0s 754us/step - loss: 527.8455 - val_loss: 1571.5135 - lr: 0.0098\n",
            "\n",
            "Epoch 224: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 224/300\n",
            "416/416 [==============================] - 0s 733us/step - loss: 533.0625 - val_loss: 1569.4360 - lr: 0.0098\n",
            "\n",
            "Epoch 225: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 225/300\n",
            "416/416 [==============================] - 0s 757us/step - loss: 535.1800 - val_loss: 1579.3813 - lr: 0.0098\n",
            "\n",
            "Epoch 226: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 226/300\n",
            "416/416 [==============================] - 0s 730us/step - loss: 530.2407 - val_loss: 1585.9294 - lr: 0.0098\n",
            "\n",
            "Epoch 227: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 227/300\n",
            "416/416 [==============================] - 0s 745us/step - loss: 529.4710 - val_loss: 1578.9811 - lr: 0.0098\n",
            "\n",
            "Epoch 228: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 228/300\n",
            "416/416 [==============================] - 0s 750us/step - loss: 533.2013 - val_loss: 1669.8286 - lr: 0.0098\n",
            "\n",
            "Epoch 229: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 229/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 531.2942 - val_loss: 1593.0044 - lr: 0.0098\n",
            "\n",
            "Epoch 230: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 230/300\n",
            "416/416 [==============================] - 0s 783us/step - loss: 532.7072 - val_loss: 1633.7081 - lr: 0.0098\n",
            "\n",
            "Epoch 231: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 231/300\n",
            "416/416 [==============================] - 0s 757us/step - loss: 528.5564 - val_loss: 1588.0118 - lr: 0.0089\n",
            "\n",
            "Epoch 232: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 232/300\n",
            "416/416 [==============================] - 0s 733us/step - loss: 526.3286 - val_loss: 1590.5312 - lr: 0.0089\n",
            "\n",
            "Epoch 233: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 233/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 525.9874 - val_loss: 1598.1659 - lr: 0.0089\n",
            "\n",
            "Epoch 234: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 234/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 526.9224 - val_loss: 1582.5803 - lr: 0.0089\n",
            "\n",
            "Epoch 235: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 235/300\n",
            "416/416 [==============================] - 0s 730us/step - loss: 523.8414 - val_loss: 1595.5320 - lr: 0.0089\n",
            "\n",
            "Epoch 236: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 236/300\n",
            "416/416 [==============================] - 0s 730us/step - loss: 526.9459 - val_loss: 1597.7310 - lr: 0.0089\n",
            "\n",
            "Epoch 237: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 237/300\n",
            "416/416 [==============================] - 0s 769us/step - loss: 529.5424 - val_loss: 1578.8433 - lr: 0.0089\n",
            "\n",
            "Epoch 238: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 238/300\n",
            "416/416 [==============================] - 0s 735us/step - loss: 528.0671 - val_loss: 1624.9032 - lr: 0.0089\n",
            "\n",
            "Epoch 239: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 239/300\n",
            "416/416 [==============================] - 0s 740us/step - loss: 528.3522 - val_loss: 1581.1417 - lr: 0.0089\n",
            "\n",
            "Epoch 240: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 240/300\n",
            "416/416 [==============================] - 0s 762us/step - loss: 533.0241 - val_loss: 1571.8987 - lr: 0.0089\n",
            "\n",
            "Epoch 241: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 241/300\n",
            "416/416 [==============================] - 0s 725us/step - loss: 523.2486 - val_loss: 1572.8245 - lr: 0.0080\n",
            "\n",
            "Epoch 242: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 242/300\n",
            "416/416 [==============================] - 0s 730us/step - loss: 527.4640 - val_loss: 1572.2697 - lr: 0.0080\n",
            "\n",
            "Epoch 243: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 243/300\n",
            "416/416 [==============================] - 0s 754us/step - loss: 523.2020 - val_loss: 1647.9043 - lr: 0.0080\n",
            "\n",
            "Epoch 244: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 244/300\n",
            "416/416 [==============================] - 0s 730us/step - loss: 520.6071 - val_loss: 1608.0801 - lr: 0.0080\n",
            "\n",
            "Epoch 245: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 245/300\n",
            "416/416 [==============================] - 0s 762us/step - loss: 521.6246 - val_loss: 1575.7092 - lr: 0.0080\n",
            "\n",
            "Epoch 246: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 246/300\n",
            "416/416 [==============================] - 0s 740us/step - loss: 525.6305 - val_loss: 1587.4874 - lr: 0.0080\n",
            "\n",
            "Epoch 247: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 247/300\n",
            "416/416 [==============================] - 0s 745us/step - loss: 524.3362 - val_loss: 1624.3044 - lr: 0.0080\n",
            "\n",
            "Epoch 248: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 248/300\n",
            "416/416 [==============================] - 0s 740us/step - loss: 524.3256 - val_loss: 1592.9967 - lr: 0.0080\n",
            "\n",
            "Epoch 249: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 249/300\n",
            "416/416 [==============================] - 0s 728us/step - loss: 525.1688 - val_loss: 1569.8457 - lr: 0.0080\n",
            "\n",
            "Epoch 250: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 250/300\n",
            "416/416 [==============================] - 0s 754us/step - loss: 524.3168 - val_loss: 1581.3079 - lr: 0.0080\n",
            "\n",
            "Epoch 251: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 251/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 518.3478 - val_loss: 1582.6942 - lr: 0.0072\n",
            "\n",
            "Epoch 252: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 252/300\n",
            "416/416 [==============================] - 0s 757us/step - loss: 519.4505 - val_loss: 1589.3638 - lr: 0.0072\n",
            "\n",
            "Epoch 253: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 253/300\n",
            "416/416 [==============================] - 0s 778us/step - loss: 521.1166 - val_loss: 1588.2786 - lr: 0.0072\n",
            "\n",
            "Epoch 254: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 254/300\n",
            "416/416 [==============================] - 0s 764us/step - loss: 520.0609 - val_loss: 1609.3195 - lr: 0.0072\n",
            "\n",
            "Epoch 255: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 255/300\n",
            "416/416 [==============================] - 0s 745us/step - loss: 519.7906 - val_loss: 1580.6177 - lr: 0.0072\n",
            "\n",
            "Epoch 256: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 256/300\n",
            "416/416 [==============================] - 0s 733us/step - loss: 520.0670 - val_loss: 1576.4048 - lr: 0.0072\n",
            "\n",
            "Epoch 257: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 257/300\n",
            "416/416 [==============================] - 0s 759us/step - loss: 519.8043 - val_loss: 1607.5187 - lr: 0.0072\n",
            "\n",
            "Epoch 258: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 258/300\n",
            "416/416 [==============================] - 0s 764us/step - loss: 521.0679 - val_loss: 1585.2968 - lr: 0.0072\n",
            "\n",
            "Epoch 259: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 259/300\n",
            "416/416 [==============================] - 0s 738us/step - loss: 519.8150 - val_loss: 1583.1600 - lr: 0.0072\n",
            "\n",
            "Epoch 260: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 260/300\n",
            "416/416 [==============================] - 0s 783us/step - loss: 519.9703 - val_loss: 1597.8607 - lr: 0.0072\n",
            "\n",
            "Epoch 261: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 261/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 517.5973 - val_loss: 1575.9462 - lr: 0.0065\n",
            "\n",
            "Epoch 262: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 262/300\n",
            "416/416 [==============================] - 0s 791us/step - loss: 516.5511 - val_loss: 1590.2845 - lr: 0.0065\n",
            "\n",
            "Epoch 263: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 263/300\n",
            "416/416 [==============================] - 0s 762us/step - loss: 517.5038 - val_loss: 1586.5601 - lr: 0.0065\n",
            "\n",
            "Epoch 264: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 264/300\n",
            "416/416 [==============================] - 0s 735us/step - loss: 518.4846 - val_loss: 1585.0952 - lr: 0.0065\n",
            "\n",
            "Epoch 265: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 265/300\n",
            "416/416 [==============================] - 0s 783us/step - loss: 515.8878 - val_loss: 1601.6387 - lr: 0.0065\n",
            "\n",
            "Epoch 266: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 266/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 516.7996 - val_loss: 1584.6570 - lr: 0.0065\n",
            "\n",
            "Epoch 267: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 267/300\n",
            "416/416 [==============================] - 0s 774us/step - loss: 518.6642 - val_loss: 1585.9310 - lr: 0.0065\n",
            "\n",
            "Epoch 268: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 268/300\n",
            "416/416 [==============================] - 0s 723us/step - loss: 516.3225 - val_loss: 1585.9983 - lr: 0.0065\n",
            "\n",
            "Epoch 269: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 269/300\n",
            "416/416 [==============================] - 0s 740us/step - loss: 514.0561 - val_loss: 1619.1698 - lr: 0.0065\n",
            "\n",
            "Epoch 270: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 270/300\n",
            "416/416 [==============================] - 0s 747us/step - loss: 516.8629 - val_loss: 1587.8484 - lr: 0.0065\n",
            "\n",
            "Epoch 271: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 271/300\n",
            "416/416 [==============================] - 0s 740us/step - loss: 513.7974 - val_loss: 1575.6775 - lr: 0.0058\n",
            "\n",
            "Epoch 272: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 272/300\n",
            "416/416 [==============================] - 0s 733us/step - loss: 518.3920 - val_loss: 1625.8021 - lr: 0.0058\n",
            "\n",
            "Epoch 273: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 273/300\n",
            "416/416 [==============================] - 0s 774us/step - loss: 513.8625 - val_loss: 1581.7850 - lr: 0.0058\n",
            "\n",
            "Epoch 274: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 274/300\n",
            "416/416 [==============================] - 0s 774us/step - loss: 513.9861 - val_loss: 1592.8289 - lr: 0.0058\n",
            "\n",
            "Epoch 275: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 275/300\n",
            "416/416 [==============================] - 0s 740us/step - loss: 513.1591 - val_loss: 1581.9360 - lr: 0.0058\n",
            "\n",
            "Epoch 276: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 276/300\n",
            "416/416 [==============================] - 0s 754us/step - loss: 515.4257 - val_loss: 1602.6150 - lr: 0.0058\n",
            "\n",
            "Epoch 277: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 277/300\n",
            "416/416 [==============================] - 0s 752us/step - loss: 515.3445 - val_loss: 1585.4722 - lr: 0.0058\n",
            "\n",
            "Epoch 278: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 278/300\n",
            "416/416 [==============================] - 0s 742us/step - loss: 516.8433 - val_loss: 1577.4559 - lr: 0.0058\n",
            "\n",
            "Epoch 279: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 279/300\n",
            "416/416 [==============================] - 0s 745us/step - loss: 517.8054 - val_loss: 1583.8197 - lr: 0.0058\n",
            "\n",
            "Epoch 280: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 280/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 517.8281 - val_loss: 1607.9623 - lr: 0.0058\n",
            "\n",
            "Epoch 281: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 281/300\n",
            "416/416 [==============================] - 0s 730us/step - loss: 512.6542 - val_loss: 1584.4675 - lr: 0.0052\n",
            "\n",
            "Epoch 282: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 282/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 511.0226 - val_loss: 1584.5717 - lr: 0.0052\n",
            "\n",
            "Epoch 283: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 283/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 512.8987 - val_loss: 1576.7351 - lr: 0.0052\n",
            "\n",
            "Epoch 284: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 284/300\n",
            "416/416 [==============================] - 0s 763us/step - loss: 514.5495 - val_loss: 1573.5070 - lr: 0.0052\n",
            "\n",
            "Epoch 285: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 285/300\n",
            "416/416 [==============================] - 0s 745us/step - loss: 513.0887 - val_loss: 1580.8008 - lr: 0.0052\n",
            "\n",
            "Epoch 286: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 286/300\n",
            "416/416 [==============================] - 0s 778us/step - loss: 514.5456 - val_loss: 1596.6060 - lr: 0.0052\n",
            "\n",
            "Epoch 287: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 287/300\n",
            "416/416 [==============================] - 0s 732us/step - loss: 511.4278 - val_loss: 1590.2279 - lr: 0.0052\n",
            "\n",
            "Epoch 288: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 288/300\n",
            "416/416 [==============================] - 0s 725us/step - loss: 513.1465 - val_loss: 1587.5656 - lr: 0.0052\n",
            "\n",
            "Epoch 289: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 289/300\n",
            "416/416 [==============================] - 0s 740us/step - loss: 513.9645 - val_loss: 1617.3767 - lr: 0.0052\n",
            "\n",
            "Epoch 290: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 290/300\n",
            "416/416 [==============================] - 0s 733us/step - loss: 511.6616 - val_loss: 1604.2135 - lr: 0.0052\n",
            "\n",
            "Epoch 291: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 291/300\n",
            "416/416 [==============================] - 0s 730us/step - loss: 510.8112 - val_loss: 1584.4573 - lr: 0.0047\n",
            "\n",
            "Epoch 292: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 292/300\n",
            "416/416 [==============================] - 0s 774us/step - loss: 511.9704 - val_loss: 1579.6685 - lr: 0.0047\n",
            "\n",
            "Epoch 293: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 293/300\n",
            "416/416 [==============================] - 0s 754us/step - loss: 510.7050 - val_loss: 1589.8993 - lr: 0.0047\n",
            "\n",
            "Epoch 294: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 294/300\n",
            "416/416 [==============================] - 0s 740us/step - loss: 511.0170 - val_loss: 1585.7583 - lr: 0.0047\n",
            "\n",
            "Epoch 295: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 295/300\n",
            "416/416 [==============================] - 0s 735us/step - loss: 510.7729 - val_loss: 1588.9846 - lr: 0.0047\n",
            "\n",
            "Epoch 296: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 296/300\n",
            "416/416 [==============================] - 0s 750us/step - loss: 510.5554 - val_loss: 1597.7870 - lr: 0.0047\n",
            "\n",
            "Epoch 297: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 297/300\n",
            "416/416 [==============================] - 0s 740us/step - loss: 509.6476 - val_loss: 1576.0310 - lr: 0.0047\n",
            "\n",
            "Epoch 298: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 298/300\n",
            "416/416 [==============================] - 0s 742us/step - loss: 510.2994 - val_loss: 1584.1814 - lr: 0.0047\n",
            "\n",
            "Epoch 299: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 299/300\n",
            "416/416 [==============================] - 0s 783us/step - loss: 510.9688 - val_loss: 1582.9607 - lr: 0.0047\n",
            "\n",
            "Epoch 300: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 300/300\n",
            "416/416 [==============================] - 0s 752us/step - loss: 510.1985 - val_loss: 1592.5894 - lr: 0.0047\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import math\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=92, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='relu'))\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    initial_learning_rate = 0.1\n",
        "    decay_rate = 0.9\n",
        "    epoch_rate = 10\n",
        "    return initial_learning_rate * math.pow(decay_rate, math.floor(epoch/epoch_rate))\n",
        "\n",
        "# Compile the model with your desired optimizer and loss function\n",
        "optimizer = Adam(learning_rate=0.1)  # Set initial learning rate\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "# Create a learning rate callback\n",
        "lr_callback = LearningRateScheduler(lr_schedule, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 300, callbacks=[lr_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "416/416 [==============================] - 0s 545us/step - loss: 496.1009\n",
            "104/104 [==============================] - 0s 515us/step - loss: 1592.5894\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the training data\n",
        "train_mse = model.evaluate(X_train, y_train, verbose = 1)\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "test_mse = model.evaluate(X_test, y_test, verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "104/104 [==============================] - 0s 495us/step\n",
            "0.9498959587123107\n"
          ]
        }
      ],
      "source": [
        "# Get predictions for the testing data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Get the r^2\n",
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x24f42c13d90>"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot loss during training (we can do this because we saved a \"history\" during training)\n",
        "from matplotlib import pyplot\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Loss')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOaeajgRDKbWJPJErwu30Xr",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
