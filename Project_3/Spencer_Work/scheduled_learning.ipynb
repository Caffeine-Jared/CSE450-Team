{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Caffeine-Jared/CSE450-Team/blob/main/Initial_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w8AJLFDq_-Er"
      },
      "source": [
        "Initial Model - Working through the example code that was provided by the professor."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KSe9ZbMDBIka"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4ggu-08_3mF",
        "outputId": "6993f428-f72e-4c0b-dec5-0519b65e6523"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 16637 entries, 0 to 16636\n",
            "Data columns (total 12 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   dteday        16637 non-null  object \n",
            " 1   season        16637 non-null  int64  \n",
            " 2   hr            16637 non-null  int64  \n",
            " 3   holiday       16637 non-null  int64  \n",
            " 4   workingday    16637 non-null  int64  \n",
            " 5   weathersit    16637 non-null  int64  \n",
            " 6   hum           16637 non-null  float64\n",
            " 7   windspeed     16637 non-null  int64  \n",
            " 8   temp_c        16637 non-null  float64\n",
            " 9   feels_like_c  16637 non-null  float64\n",
            " 10  casual        16637 non-null  int64  \n",
            " 11  registered    16637 non-null  int64  \n",
            "dtypes: float64(3), int64(8), object(1)\n",
            "memory usage: 1.5+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# loading in the bikes csv\n",
        "bikes_df = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')\n",
        "bikes_mini = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/biking_holdout_test_mini.csv')\n",
        "bikes_holdout = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes_december.csv')\n",
        "\n",
        "# check out the info\n",
        "bikes_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>3.28</td>\n",
              "      <td>3.0014</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0</td>\n",
              "      <td>2.34</td>\n",
              "      <td>1.9982</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0</td>\n",
              "      <td>2.34</td>\n",
              "      <td>1.9982</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0</td>\n",
              "      <td>3.28</td>\n",
              "      <td>3.0014</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0</td>\n",
              "      <td>3.28</td>\n",
              "      <td>3.0014</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dteday  season  hr  holiday  workingday  weathersit   hum  windspeed   \n",
              "0  1/1/11       1   0        0           0           1  0.81          0  \\\n",
              "1  1/1/11       1   1        0           0           1  0.80          0   \n",
              "2  1/1/11       1   2        0           0           1  0.80          0   \n",
              "3  1/1/11       1   3        0           0           1  0.75          0   \n",
              "4  1/1/11       1   4        0           0           1  0.75          0   \n",
              "\n",
              "   temp_c  feels_like_c  casual  registered  \n",
              "0    3.28        3.0014       3          13  \n",
              "1    2.34        1.9982       8          32  \n",
              "2    2.34        1.9982       5          27  \n",
              "3    3.28        3.0014       3          10  \n",
              "4    3.28        3.0014       0           1  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bikes_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ifhWb55CmrN",
        "outputId": "35c9566b-913e-4dff-b500-530554f35f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35 entries, 0 to 34\n",
            "Data columns (total 10 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   dteday        35 non-null     object \n",
            " 1   season        35 non-null     int64  \n",
            " 2   hr            35 non-null     int64  \n",
            " 3   holiday       35 non-null     int64  \n",
            " 4   workingday    35 non-null     int64  \n",
            " 5   weathersit    35 non-null     int64  \n",
            " 6   hum           35 non-null     float64\n",
            " 7   windspeed     35 non-null     int64  \n",
            " 8   temp_c        35 non-null     float64\n",
            " 9   feels_like_c  35 non-null     float64\n",
            "dtypes: float64(3), int64(6), object(1)\n",
            "memory usage: 2.9+ KB\n"
          ]
        }
      ],
      "source": [
        "bikes_mini.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.81</td>\n",
              "      <td>9</td>\n",
              "      <td>4.22</td>\n",
              "      <td>1.9982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>6</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.0014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    dteday  season  hr  holiday  workingday  weathersit   hum  windspeed   \n",
              "0  12/1/12       4   0        0           0           1  0.81          0  \\\n",
              "1  12/1/12       4   1        0           0           1  0.81          0   \n",
              "2  12/1/12       4   2        0           0           2  0.81          0   \n",
              "3  12/1/12       4   3        0           0           2  0.81          9   \n",
              "4  12/1/12       4   4        0           0           1  0.81          6   \n",
              "\n",
              "   temp_c  feels_like_c  \n",
              "0    4.22        3.9980  \n",
              "1    4.22        3.9980  \n",
              "2    4.22        3.9980  \n",
              "3    4.22        1.9982  \n",
              "4    4.22        3.0014  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bikes_mini.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAVPjqQ0Cqqf",
        "outputId": "c18cf7f5-9b43-4fdf-b1f1-09c793bb0ae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 742 entries, 0 to 741\n",
            "Data columns (total 10 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   dteday        742 non-null    object \n",
            " 1   season        742 non-null    int64  \n",
            " 2   hr            742 non-null    int64  \n",
            " 3   holiday       742 non-null    int64  \n",
            " 4   workingday    742 non-null    int64  \n",
            " 5   weathersit    742 non-null    int64  \n",
            " 6   hum           742 non-null    float64\n",
            " 7   windspeed     742 non-null    int64  \n",
            " 8   temp_c        742 non-null    float64\n",
            " 9   feels_like_c  742 non-null    float64\n",
            "dtypes: float64(3), int64(6), object(1)\n",
            "memory usage: 58.1+ KB\n"
          ]
        }
      ],
      "source": [
        "bikes_holdout.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.81</td>\n",
              "      <td>9</td>\n",
              "      <td>4.22</td>\n",
              "      <td>1.9982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>6</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.0014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    dteday  season  hr  holiday  workingday  weathersit   hum  windspeed   \n",
              "0  12/1/12       4   0        0           0           1  0.81          0  \\\n",
              "1  12/1/12       4   1        0           0           1  0.81          0   \n",
              "2  12/1/12       4   2        0           0           2  0.81          0   \n",
              "3  12/1/12       4   3        0           0           2  0.81          9   \n",
              "4  12/1/12       4   4        0           0           1  0.81          6   \n",
              "\n",
              "   temp_c  feels_like_c  \n",
              "0    4.22        3.9980  \n",
              "1    4.22        3.9980  \n",
              "2    4.22        3.9980  \n",
              "3    4.22        1.9982  \n",
              "4    4.22        3.0014  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bikes_holdout.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VlXM5n8TGNLM"
      },
      "outputs": [],
      "source": [
        "# Preprocessing \n",
        "bikes_df['dteday'] = pd.to_datetime(bikes_df['dteday'], format='%m/%d/%y')\n",
        "bikes_mini['dteday'] = pd.to_datetime(bikes_mini['dteday'], format='%m/%d/%y')\n",
        "bikes_holdout['dteday'] = pd.to_datetime(bikes_holdout['dteday'], format='%m/%d/%y')\n",
        "\n",
        "# create new features - year, month, day, dayofweek\n",
        "bikes_df['year'] = pd.to_datetime(bikes_df['dteday']).dt.year\n",
        "bikes_df['month'] = pd.to_datetime(bikes_df['dteday']).dt.month\n",
        "bikes_df['day'] = pd.to_datetime(bikes_df['dteday']).dt.day\n",
        "bikes_df['dayofweek'] = pd.to_datetime(bikes_df['dteday']).dt.dayofweek\n",
        "\n",
        "bikes_mini['year'] = pd.to_datetime(bikes_mini['dteday']).dt.year\n",
        "bikes_mini['month'] = pd.to_datetime(bikes_mini['dteday']).dt.month\n",
        "bikes_mini['day'] = pd.to_datetime(bikes_mini['dteday']).dt.day\n",
        "bikes_mini['dayofweek'] = pd.to_datetime(bikes_mini['dteday']).dt.dayofweek\n",
        "\n",
        "bikes_holdout['year'] = pd.to_datetime(bikes_holdout['dteday']).dt.year\n",
        "bikes_holdout['month'] = pd.to_datetime(bikes_holdout['dteday']).dt.month\n",
        "bikes_holdout['day'] = pd.to_datetime(bikes_holdout['dteday']).dt.day\n",
        "bikes_holdout['dayofweek'] = pd.to_datetime(bikes_holdout['dteday']).dt.dayofweek\n",
        "\n",
        "# drop dteday column\n",
        "bikes_df = bikes_df.drop('dteday', axis=1)\n",
        "bikes_mini = bikes_mini.drop('dteday', axis=1)\n",
        "bikes_holdout = bikes_holdout.drop('dteday', axis=1)\n",
        "\n",
        "# one hot encoding\n",
        "categorical_features = ['season', 'hr', 'holiday', 'workingday', 'weathersit', 'year', 'month', 'day', 'dayofweek']\n",
        "bikes_df = pd.get_dummies(bikes_df, columns=categorical_features, dtype=int)\n",
        "bikes_mini = pd.get_dummies(bikes_mini, columns=categorical_features, dtype=int)\n",
        "bikes_holdout = pd.get_dummies(bikes_holdout, columns=categorical_features, dtype=int)\n",
        "\n",
        "# min max scaling\n",
        "scaler = MinMaxScaler()\n",
        "bikes_df[['temp_c', 'hum', 'feels_like_c', 'windspeed']] = scaler.fit_transform(bikes_df[['temp_c', 'hum', 'feels_like_c', 'windspeed']])\n",
        "bikes_mini[['temp_c', 'hum', 'feels_like_c', 'windspeed']] = scaler.fit_transform(bikes_mini[['temp_c', 'hum', 'feels_like_c', 'windspeed']])\n",
        "bikes_holdout[['temp_c', 'hum', 'feels_like_c', 'windspeed']] = scaler.fit_transform(bikes_holdout[['temp_c', 'hum', 'feels_like_c', 'windspeed']])\n",
        "\n",
        "# creating a total count column\n",
        "bikes_df['total_count'] = bikes_df['casual'] + bikes_df['registered']\n",
        "# when it comes to adding these columns together, we don't care about the specifics between casual and registered, we just want the total count, as it provides more information\n",
        "# additionally, the questions we need to answer surround total count, not casual or registered\n",
        "# drop casual and registered columns\n",
        "bikes_df = bikes_df.drop(columns=['casual', 'registered'])\n",
        "# it's important to scale the numbers as not scaling would cause the model to think that the total count is more important than the other features\n",
        "# scale bikes_df total_count\n",
        "#bikes_df[['total_count']] = scaler.fit_transform(bikes_df[['total_count']])\n",
        "# features and the target\n",
        "X = bikes_df.drop(columns=['total_count'])\n",
        "y = bikes_df['total_count']\n",
        "\n",
        "# training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "      <th>season_1</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>season_4</th>\n",
              "      <th>hr_0</th>\n",
              "      <th>hr_1</th>\n",
              "      <th>...</th>\n",
              "      <th>day_29</th>\n",
              "      <th>day_30</th>\n",
              "      <th>day_31</th>\n",
              "      <th>dayofweek_0</th>\n",
              "      <th>dayofweek_1</th>\n",
              "      <th>dayofweek_2</th>\n",
              "      <th>dayofweek_3</th>\n",
              "      <th>dayofweek_4</th>\n",
              "      <th>dayofweek_5</th>\n",
              "      <th>dayofweek_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14616</th>\n",
              "      <td>0.84</td>\n",
              "      <td>0.228070</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.6667</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4605</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.263158</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.6364</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13893</th>\n",
              "      <td>0.84</td>\n",
              "      <td>0.157895</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.6667</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2251</th>\n",
              "      <td>0.93</td>\n",
              "      <td>0.298246</td>\n",
              "      <td>0.306122</td>\n",
              "      <td>0.3030</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.192982</td>\n",
              "      <td>0.183673</td>\n",
              "      <td>0.2121</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 92 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        hum  windspeed    temp_c  feels_like_c  season_1  season_2  season_3   \n",
              "14616  0.84   0.228070  0.693878        0.6667         0         0         1  \\\n",
              "4605   0.45   0.263158  0.693878        0.6364         0         0         1   \n",
              "13893  0.84   0.157895  0.693878        0.6667         0         0         1   \n",
              "2251   0.93   0.298246  0.306122        0.3030         0         1         0   \n",
              "1999   0.44   0.192982  0.183673        0.2121         0         1         0   \n",
              "\n",
              "       season_4  hr_0  hr_1  ...  day_29  day_30  day_31  dayofweek_0   \n",
              "14616         0     0     0  ...       0       0       0            0  \\\n",
              "4605          0     0     0  ...       0       0       0            0   \n",
              "13893         0     0     0  ...       0       0       0            0   \n",
              "2251          0     0     0  ...       0       0       0            0   \n",
              "1999          0     0     0  ...       1       0       0            0   \n",
              "\n",
              "       dayofweek_1  dayofweek_2  dayofweek_3  dayofweek_4  dayofweek_5   \n",
              "14616            0            0            1            0            0  \\\n",
              "4605             0            0            0            1            0   \n",
              "13893            1            0            0            0            0   \n",
              "2251             0            0            0            1            0   \n",
              "1999             1            0            0            0            0   \n",
              "\n",
              "       dayofweek_6  \n",
              "14616            0  \n",
              "4605             0  \n",
              "13893            0  \n",
              "2251             0  \n",
              "1999             0  \n",
              "\n",
              "[5 rows x 92 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "      <th>season_1</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>season_4</th>\n",
              "      <th>hr_0</th>\n",
              "      <th>hr_1</th>\n",
              "      <th>...</th>\n",
              "      <th>day_29</th>\n",
              "      <th>day_30</th>\n",
              "      <th>day_31</th>\n",
              "      <th>dayofweek_0</th>\n",
              "      <th>dayofweek_1</th>\n",
              "      <th>dayofweek_2</th>\n",
              "      <th>dayofweek_3</th>\n",
              "      <th>dayofweek_4</th>\n",
              "      <th>dayofweek_5</th>\n",
              "      <th>dayofweek_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.081633</td>\n",
              "      <td>0.0909</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10235</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.491228</td>\n",
              "      <td>0.489796</td>\n",
              "      <td>0.4848</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12309</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.530612</td>\n",
              "      <td>0.5152</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8158</th>\n",
              "      <td>0.28</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.3333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12027</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.298246</td>\n",
              "      <td>0.551020</td>\n",
              "      <td>0.5303</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 92 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        hum  windspeed    temp_c  feels_like_c  season_1  season_2  season_3   \n",
              "191    0.49   0.333333  0.081633        0.0909         1         0         0  \\\n",
              "10235  0.34   0.491228  0.489796        0.4848         1         0         0   \n",
              "12309  0.88   0.000000  0.530612        0.5152         0         1         0   \n",
              "8158   0.28   0.000000  0.285714        0.3333         0         0         0   \n",
              "12027  0.88   0.298246  0.551020        0.5303         0         1         0   \n",
              "\n",
              "       season_4  hr_0  hr_1  ...  day_29  day_30  day_31  dayofweek_0   \n",
              "191           0     0     0  ...       0       0       0            0  \\\n",
              "10235         0     0     0  ...       0       0       0            0   \n",
              "12309         0     0     0  ...       0       0       0            0   \n",
              "8158          1     0     0  ...       0       0       0            0   \n",
              "12027         0     0     0  ...       0       0       0            1   \n",
              "\n",
              "       dayofweek_1  dayofweek_2  dayofweek_3  dayofweek_4  dayofweek_5   \n",
              "191              0            0            0            0            0  \\\n",
              "10235            0            1            0            0            0   \n",
              "12309            0            0            0            0            1   \n",
              "8158             0            0            0            0            0   \n",
              "12027            0            0            0            0            0   \n",
              "\n",
              "       dayofweek_6  \n",
              "191              1  \n",
              "10235            0  \n",
              "12309            0  \n",
              "8158             1  \n",
              "12027            0  \n",
              "\n",
              "[5 rows x 92 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "      <th>season_1</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>season_4</th>\n",
              "      <th>hr_0</th>\n",
              "      <th>hr_1</th>\n",
              "      <th>...</th>\n",
              "      <th>day_30</th>\n",
              "      <th>day_31</th>\n",
              "      <th>dayofweek_0</th>\n",
              "      <th>dayofweek_1</th>\n",
              "      <th>dayofweek_2</th>\n",
              "      <th>dayofweek_3</th>\n",
              "      <th>dayofweek_4</th>\n",
              "      <th>dayofweek_5</th>\n",
              "      <th>dayofweek_6</th>\n",
              "      <th>total_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.224490</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.204082</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.204082</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.224490</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.224490</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 93 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    hum  windspeed    temp_c  feels_like_c  season_1  season_2  season_3   \n",
              "0  0.81        0.0  0.224490        0.2879         1         0         0  \\\n",
              "1  0.80        0.0  0.204082        0.2727         1         0         0   \n",
              "2  0.80        0.0  0.204082        0.2727         1         0         0   \n",
              "3  0.75        0.0  0.224490        0.2879         1         0         0   \n",
              "4  0.75        0.0  0.224490        0.2879         1         0         0   \n",
              "\n",
              "   season_4  hr_0  hr_1  ...  day_30  day_31  dayofweek_0  dayofweek_1   \n",
              "0         0     1     0  ...       0       0            0            0  \\\n",
              "1         0     0     1  ...       0       0            0            0   \n",
              "2         0     0     0  ...       0       0            0            0   \n",
              "3         0     0     0  ...       0       0            0            0   \n",
              "4         0     0     0  ...       0       0            0            0   \n",
              "\n",
              "   dayofweek_2  dayofweek_3  dayofweek_4  dayofweek_5  dayofweek_6   \n",
              "0            0            0            0            1            0  \\\n",
              "1            0            0            0            1            0   \n",
              "2            0            0            0            1            0   \n",
              "3            0            0            0            1            0   \n",
              "4            0            0            0            1            0   \n",
              "\n",
              "   total_count  \n",
              "0           16  \n",
              "1           40  \n",
              "2           32  \n",
              "3           13  \n",
              "4            1  \n",
              "\n",
              "[5 rows x 93 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bikes_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13309, 92)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "bikes_df.head()\n",
        "bikes_df.to_csv('bikes_output.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 1/300\n",
            "416/416 [==============================] - 1s 950us/step - loss: 7458.9971 - val_loss: 4174.9619 - lr: 0.1000\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 2/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 3283.5723 - val_loss: 2780.0400 - lr: 0.1000\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 3/300\n",
            "416/416 [==============================] - 0s 815us/step - loss: 2904.8203 - val_loss: 3131.8669 - lr: 0.1000\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 4/300\n",
            "416/416 [==============================] - 0s 791us/step - loss: 2604.5542 - val_loss: 2436.6868 - lr: 0.1000\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 5/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 2491.1335 - val_loss: 2509.7329 - lr: 0.1000\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 6/300\n",
            "416/416 [==============================] - 0s 786us/step - loss: 2245.1833 - val_loss: 2134.5032 - lr: 0.1000\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 7/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 2006.3512 - val_loss: 2632.1575 - lr: 0.1000\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 8/300\n",
            "416/416 [==============================] - 0s 803us/step - loss: 2040.7809 - val_loss: 2066.9229 - lr: 0.1000\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 9/300\n",
            "416/416 [==============================] - 0s 783us/step - loss: 1891.9712 - val_loss: 2645.5847 - lr: 0.1000\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 10/300\n",
            "416/416 [==============================] - 0s 781us/step - loss: 1695.4016 - val_loss: 2514.1362 - lr: 0.1000\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 11/300\n",
            "416/416 [==============================] - 0s 815us/step - loss: 1633.2606 - val_loss: 2476.3977 - lr: 0.0900\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 12/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 1493.2760 - val_loss: 2501.2354 - lr: 0.0900\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 13/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 1493.0126 - val_loss: 2006.9558 - lr: 0.0900\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 14/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 1535.3495 - val_loss: 2185.1040 - lr: 0.0900\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 15/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 1534.4832 - val_loss: 2259.6187 - lr: 0.0900\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 16/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 1380.4189 - val_loss: 1674.3418 - lr: 0.0900\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 17/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 1385.9143 - val_loss: 1910.8231 - lr: 0.0900\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 18/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 1287.5344 - val_loss: 1744.7079 - lr: 0.0900\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 19/300\n",
            "416/416 [==============================] - 0s 793us/step - loss: 1327.9973 - val_loss: 1812.9093 - lr: 0.0900\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 20/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 1339.1924 - val_loss: 1661.5529 - lr: 0.0900\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 21/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 1210.6304 - val_loss: 1889.7305 - lr: 0.0810\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 22/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 1170.8230 - val_loss: 1722.4832 - lr: 0.0810\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 23/300\n",
            "416/416 [==============================] - 0s 815us/step - loss: 1109.0930 - val_loss: 1717.3921 - lr: 0.0810\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 24/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 1212.0859 - val_loss: 1797.3640 - lr: 0.0810\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 25/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 1137.3318 - val_loss: 1764.5544 - lr: 0.0810\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 26/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 1123.2064 - val_loss: 2074.3284 - lr: 0.0810\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 27/300\n",
            "416/416 [==============================] - 0s 791us/step - loss: 1155.4272 - val_loss: 1885.4406 - lr: 0.0810\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 28/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 1131.9009 - val_loss: 2022.5753 - lr: 0.0810\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 29/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 1050.8778 - val_loss: 1721.9751 - lr: 0.0810\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 30/300\n",
            "416/416 [==============================] - 0s 793us/step - loss: 1111.4905 - val_loss: 2154.3091 - lr: 0.0810\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 31/300\n",
            "416/416 [==============================] - 0s 791us/step - loss: 1005.3788 - val_loss: 1678.5945 - lr: 0.0729\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 32/300\n",
            "416/416 [==============================] - 0s 791us/step - loss: 1024.0365 - val_loss: 1774.9376 - lr: 0.0729\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 33/300\n",
            "416/416 [==============================] - 0s 791us/step - loss: 983.3973 - val_loss: 1736.8783 - lr: 0.0729\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 34/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 1015.6312 - val_loss: 1855.1052 - lr: 0.0729\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 35/300\n",
            "416/416 [==============================] - 0s 793us/step - loss: 1089.2382 - val_loss: 1736.3074 - lr: 0.0729\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 36/300\n",
            "416/416 [==============================] - 0s 803us/step - loss: 994.0334 - val_loss: 2034.4745 - lr: 0.0729\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 37/300\n",
            "416/416 [==============================] - 0s 783us/step - loss: 961.6481 - val_loss: 1652.4954 - lr: 0.0729\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 38/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 958.9929 - val_loss: 1772.1281 - lr: 0.0729\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 39/300\n",
            "416/416 [==============================] - 0s 793us/step - loss: 950.7681 - val_loss: 1753.9019 - lr: 0.0729\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 40/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 928.1577 - val_loss: 1977.9934 - lr: 0.0729\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 41/300\n",
            "416/416 [==============================] - 0s 803us/step - loss: 909.2338 - val_loss: 2172.5034 - lr: 0.0656\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 42/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 922.1036 - val_loss: 1802.9526 - lr: 0.0656\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 43/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 869.8345 - val_loss: 1715.8108 - lr: 0.0656\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 44/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 934.4772 - val_loss: 1683.9648 - lr: 0.0656\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 45/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 872.9643 - val_loss: 1685.2811 - lr: 0.0656\n",
            "\n",
            "Epoch 46: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 46/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 818.3620 - val_loss: 1647.9960 - lr: 0.0656\n",
            "\n",
            "Epoch 47: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 47/300\n",
            "416/416 [==============================] - 0s 824us/step - loss: 966.0535 - val_loss: 1715.6302 - lr: 0.0656\n",
            "\n",
            "Epoch 48: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 48/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 813.4081 - val_loss: 1700.8022 - lr: 0.0656\n",
            "\n",
            "Epoch 49: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 49/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 852.5244 - val_loss: 1701.3190 - lr: 0.0656\n",
            "\n",
            "Epoch 50: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 50/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 861.9615 - val_loss: 2798.6248 - lr: 0.0656\n",
            "\n",
            "Epoch 51: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 51/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 862.8921 - val_loss: 1795.5955 - lr: 0.0590\n",
            "\n",
            "Epoch 52: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 52/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 830.7739 - val_loss: 1941.2634 - lr: 0.0590\n",
            "\n",
            "Epoch 53: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 53/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 786.3857 - val_loss: 1735.6467 - lr: 0.0590\n",
            "\n",
            "Epoch 54: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 54/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 800.5078 - val_loss: 1769.8582 - lr: 0.0590\n",
            "\n",
            "Epoch 55: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 55/300\n",
            "416/416 [==============================] - 0s 815us/step - loss: 803.3443 - val_loss: 1782.3904 - lr: 0.0590\n",
            "\n",
            "Epoch 56: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 56/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 811.7167 - val_loss: 1888.0441 - lr: 0.0590\n",
            "\n",
            "Epoch 57: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 57/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 850.5051 - val_loss: 1784.5546 - lr: 0.0590\n",
            "\n",
            "Epoch 58: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 58/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 793.6593 - val_loss: 2374.9783 - lr: 0.0590\n",
            "\n",
            "Epoch 59: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 59/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 879.4846 - val_loss: 1710.9540 - lr: 0.0590\n",
            "\n",
            "Epoch 60: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 60/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 805.5378 - val_loss: 1863.3708 - lr: 0.0590\n",
            "\n",
            "Epoch 61: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 61/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 769.8932 - val_loss: 1733.8429 - lr: 0.0531\n",
            "\n",
            "Epoch 62: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 62/300\n",
            "416/416 [==============================] - 0s 786us/step - loss: 763.2801 - val_loss: 1784.5853 - lr: 0.0531\n",
            "\n",
            "Epoch 63: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 63/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 728.2560 - val_loss: 1852.4663 - lr: 0.0531\n",
            "\n",
            "Epoch 64: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 64/300\n",
            "416/416 [==============================] - 0s 791us/step - loss: 721.4423 - val_loss: 1891.6848 - lr: 0.0531\n",
            "\n",
            "Epoch 65: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 65/300\n",
            "416/416 [==============================] - 0s 791us/step - loss: 710.3201 - val_loss: 1683.1306 - lr: 0.0531\n",
            "\n",
            "Epoch 66: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 66/300\n",
            "416/416 [==============================] - 0s 803us/step - loss: 752.6301 - val_loss: 1787.9922 - lr: 0.0531\n",
            "\n",
            "Epoch 67: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 67/300\n",
            "416/416 [==============================] - 0s 771us/step - loss: 745.0895 - val_loss: 2112.9761 - lr: 0.0531\n",
            "\n",
            "Epoch 68: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 68/300\n",
            "416/416 [==============================] - 0s 788us/step - loss: 717.1265 - val_loss: 1766.0834 - lr: 0.0531\n",
            "\n",
            "Epoch 69: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 69/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 762.0978 - val_loss: 1869.8744 - lr: 0.0531\n",
            "\n",
            "Epoch 70: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 70/300\n",
            "416/416 [==============================] - 0s 786us/step - loss: 752.5836 - val_loss: 1693.6189 - lr: 0.0531\n",
            "\n",
            "Epoch 71: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 71/300\n",
            "416/416 [==============================] - 0s 791us/step - loss: 697.2546 - val_loss: 1747.3214 - lr: 0.0478\n",
            "\n",
            "Epoch 72: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 72/300\n",
            "416/416 [==============================] - 0s 824us/step - loss: 744.9365 - val_loss: 1808.0409 - lr: 0.0478\n",
            "\n",
            "Epoch 73: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 73/300\n",
            "416/416 [==============================] - 0s 793us/step - loss: 820.8966 - val_loss: 1884.1255 - lr: 0.0478\n",
            "\n",
            "Epoch 74: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 74/300\n",
            "416/416 [==============================] - 0s 803us/step - loss: 687.5210 - val_loss: 1718.4983 - lr: 0.0478\n",
            "\n",
            "Epoch 75: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 75/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 715.5704 - val_loss: 1741.9094 - lr: 0.0478\n",
            "\n",
            "Epoch 76: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 76/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 667.4285 - val_loss: 1785.7283 - lr: 0.0478\n",
            "\n",
            "Epoch 77: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 77/300\n",
            "416/416 [==============================] - 0s 793us/step - loss: 728.3361 - val_loss: 1737.6218 - lr: 0.0478\n",
            "\n",
            "Epoch 78: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 78/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 664.3337 - val_loss: 1744.6470 - lr: 0.0478\n",
            "\n",
            "Epoch 79: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 79/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 690.4547 - val_loss: 1727.0658 - lr: 0.0478\n",
            "\n",
            "Epoch 80: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 80/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 709.7651 - val_loss: 1929.8735 - lr: 0.0478\n",
            "\n",
            "Epoch 81: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 81/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 668.2476 - val_loss: 1723.3860 - lr: 0.0430\n",
            "\n",
            "Epoch 82: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 82/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 694.7344 - val_loss: 1735.6923 - lr: 0.0430\n",
            "\n",
            "Epoch 83: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 83/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 658.2520 - val_loss: 1653.8834 - lr: 0.0430\n",
            "\n",
            "Epoch 84: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 84/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 704.9576 - val_loss: 1960.2975 - lr: 0.0430\n",
            "\n",
            "Epoch 85: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 85/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 657.2911 - val_loss: 1746.0442 - lr: 0.0430\n",
            "\n",
            "Epoch 86: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 86/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 646.1066 - val_loss: 1772.3176 - lr: 0.0430\n",
            "\n",
            "Epoch 87: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 87/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 663.9337 - val_loss: 1728.4368 - lr: 0.0430\n",
            "\n",
            "Epoch 88: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 88/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 662.5844 - val_loss: 1819.8298 - lr: 0.0430\n",
            "\n",
            "Epoch 89: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 89/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 678.6074 - val_loss: 1719.9122 - lr: 0.0430\n",
            "\n",
            "Epoch 90: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 90/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 658.3993 - val_loss: 2048.3662 - lr: 0.0430\n",
            "\n",
            "Epoch 91: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 91/300\n",
            "416/416 [==============================] - 0s 870us/step - loss: 672.7767 - val_loss: 1739.2603 - lr: 0.0387\n",
            "\n",
            "Epoch 92: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 92/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 652.1905 - val_loss: 1780.3694 - lr: 0.0387\n",
            "\n",
            "Epoch 93: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 93/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 637.1243 - val_loss: 1761.9757 - lr: 0.0387\n",
            "\n",
            "Epoch 94: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 94/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 658.2788 - val_loss: 1733.9796 - lr: 0.0387\n",
            "\n",
            "Epoch 95: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 95/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 643.9216 - val_loss: 1717.4908 - lr: 0.0387\n",
            "\n",
            "Epoch 96: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 96/300\n",
            "416/416 [==============================] - 0s 812us/step - loss: 628.3085 - val_loss: 1769.4263 - lr: 0.0387\n",
            "\n",
            "Epoch 97: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 97/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 636.9579 - val_loss: 1901.7466 - lr: 0.0387\n",
            "\n",
            "Epoch 98: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 98/300\n",
            "416/416 [==============================] - 0s 923us/step - loss: 675.2153 - val_loss: 1697.2896 - lr: 0.0387\n",
            "\n",
            "Epoch 99: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 99/300\n",
            "416/416 [==============================] - 0s 860us/step - loss: 642.1371 - val_loss: 1759.5618 - lr: 0.0387\n",
            "\n",
            "Epoch 100: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 100/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 638.6759 - val_loss: 1742.0198 - lr: 0.0387\n",
            "\n",
            "Epoch 101: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 101/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 607.2170 - val_loss: 1791.9010 - lr: 0.0349\n",
            "\n",
            "Epoch 102: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 102/300\n",
            "416/416 [==============================] - 0s 921us/step - loss: 614.3033 - val_loss: 1733.2849 - lr: 0.0349\n",
            "\n",
            "Epoch 103: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 103/300\n",
            "416/416 [==============================] - 0s 894us/step - loss: 625.9565 - val_loss: 1835.7351 - lr: 0.0349\n",
            "\n",
            "Epoch 104: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 104/300\n",
            "416/416 [==============================] - 0s 897us/step - loss: 600.5446 - val_loss: 1761.7850 - lr: 0.0349\n",
            "\n",
            "Epoch 105: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 105/300\n",
            "416/416 [==============================] - 0s 928us/step - loss: 622.3278 - val_loss: 1767.3440 - lr: 0.0349\n",
            "\n",
            "Epoch 106: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 106/300\n",
            "416/416 [==============================] - 0s 916us/step - loss: 620.1448 - val_loss: 1776.8850 - lr: 0.0349\n",
            "\n",
            "Epoch 107: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 107/300\n",
            "416/416 [==============================] - 0s 930us/step - loss: 629.7427 - val_loss: 1873.4351 - lr: 0.0349\n",
            "\n",
            "Epoch 108: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 108/300\n",
            "416/416 [==============================] - 0s 882us/step - loss: 599.0787 - val_loss: 1802.1997 - lr: 0.0349\n",
            "\n",
            "Epoch 109: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 109/300\n",
            "416/416 [==============================] - 0s 897us/step - loss: 598.9310 - val_loss: 1944.7173 - lr: 0.0349\n",
            "\n",
            "Epoch 110: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 110/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 601.9261 - val_loss: 2027.2155 - lr: 0.0349\n",
            "\n",
            "Epoch 111: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 111/300\n",
            "416/416 [==============================] - 0s 870us/step - loss: 599.5181 - val_loss: 1908.3647 - lr: 0.0314\n",
            "\n",
            "Epoch 112: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 112/300\n",
            "416/416 [==============================] - 0s 868us/step - loss: 598.4493 - val_loss: 1740.5675 - lr: 0.0314\n",
            "\n",
            "Epoch 113: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 113/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 594.9966 - val_loss: 1825.5151 - lr: 0.0314\n",
            "\n",
            "Epoch 114: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 114/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 612.0159 - val_loss: 1760.2665 - lr: 0.0314\n",
            "\n",
            "Epoch 115: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 115/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 596.8033 - val_loss: 1792.1173 - lr: 0.0314\n",
            "\n",
            "Epoch 116: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 116/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 625.1840 - val_loss: 1719.4381 - lr: 0.0314\n",
            "\n",
            "Epoch 117: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 117/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 603.2920 - val_loss: 1783.5488 - lr: 0.0314\n",
            "\n",
            "Epoch 118: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 118/300\n",
            "416/416 [==============================] - 0s 863us/step - loss: 581.5567 - val_loss: 1776.4695 - lr: 0.0314\n",
            "\n",
            "Epoch 119: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 119/300\n",
            "416/416 [==============================] - 0s 913us/step - loss: 592.5514 - val_loss: 1842.8435 - lr: 0.0314\n",
            "\n",
            "Epoch 120: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 120/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 580.8124 - val_loss: 1777.4491 - lr: 0.0314\n",
            "\n",
            "Epoch 121: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 121/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 597.3228 - val_loss: 1771.0374 - lr: 0.0282\n",
            "\n",
            "Epoch 122: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 122/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 576.5746 - val_loss: 1810.1222 - lr: 0.0282\n",
            "\n",
            "Epoch 123: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 123/300\n",
            "416/416 [==============================] - 0s 824us/step - loss: 571.4801 - val_loss: 1781.4218 - lr: 0.0282\n",
            "\n",
            "Epoch 124: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 124/300\n",
            "416/416 [==============================] - 0s 824us/step - loss: 578.9304 - val_loss: 1768.9351 - lr: 0.0282\n",
            "\n",
            "Epoch 125: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 125/300\n",
            "416/416 [==============================] - 0s 860us/step - loss: 579.6052 - val_loss: 1788.6635 - lr: 0.0282\n",
            "\n",
            "Epoch 126: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 126/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 584.2693 - val_loss: 1923.9635 - lr: 0.0282\n",
            "\n",
            "Epoch 127: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 127/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 582.6007 - val_loss: 1826.4994 - lr: 0.0282\n",
            "\n",
            "Epoch 128: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 128/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 580.6433 - val_loss: 1855.8451 - lr: 0.0282\n",
            "\n",
            "Epoch 129: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 129/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 591.3284 - val_loss: 1796.1113 - lr: 0.0282\n",
            "\n",
            "Epoch 130: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 130/300\n",
            "416/416 [==============================] - 0s 856us/step - loss: 572.8214 - val_loss: 1819.6218 - lr: 0.0282\n",
            "\n",
            "Epoch 131: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 131/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 561.1298 - val_loss: 1944.5729 - lr: 0.0254\n",
            "\n",
            "Epoch 132: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 132/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 558.6137 - val_loss: 1807.5448 - lr: 0.0254\n",
            "\n",
            "Epoch 133: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 133/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 558.2928 - val_loss: 1780.1284 - lr: 0.0254\n",
            "\n",
            "Epoch 134: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 134/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 562.7892 - val_loss: 1784.0538 - lr: 0.0254\n",
            "\n",
            "Epoch 135: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 135/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 561.4017 - val_loss: 1866.0360 - lr: 0.0254\n",
            "\n",
            "Epoch 136: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 136/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 565.8765 - val_loss: 1792.0876 - lr: 0.0254\n",
            "\n",
            "Epoch 137: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 137/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 569.7250 - val_loss: 1774.4324 - lr: 0.0254\n",
            "\n",
            "Epoch 138: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 138/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 562.3751 - val_loss: 1785.0413 - lr: 0.0254\n",
            "\n",
            "Epoch 139: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 139/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 579.4493 - val_loss: 1836.0507 - lr: 0.0254\n",
            "\n",
            "Epoch 140: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 140/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 577.4012 - val_loss: 1827.5098 - lr: 0.0254\n",
            "\n",
            "Epoch 141: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 141/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 545.8513 - val_loss: 1774.5187 - lr: 0.0229\n",
            "\n",
            "Epoch 142: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 142/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 569.9122 - val_loss: 1825.6400 - lr: 0.0229\n",
            "\n",
            "Epoch 143: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 143/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 542.8616 - val_loss: 1879.9343 - lr: 0.0229\n",
            "\n",
            "Epoch 144: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 144/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 563.3030 - val_loss: 1886.1725 - lr: 0.0229\n",
            "\n",
            "Epoch 145: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 145/300\n",
            "416/416 [==============================] - 0s 819us/step - loss: 558.1061 - val_loss: 1882.7716 - lr: 0.0229\n",
            "\n",
            "Epoch 146: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 146/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 544.0112 - val_loss: 1822.9208 - lr: 0.0229\n",
            "\n",
            "Epoch 147: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 147/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 550.9700 - val_loss: 1813.0151 - lr: 0.0229\n",
            "\n",
            "Epoch 148: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 148/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 544.6588 - val_loss: 1811.2305 - lr: 0.0229\n",
            "\n",
            "Epoch 149: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 149/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 558.5307 - val_loss: 1949.0706 - lr: 0.0229\n",
            "\n",
            "Epoch 150: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 150/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 554.2616 - val_loss: 1819.0885 - lr: 0.0229\n",
            "\n",
            "Epoch 151: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 151/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 545.9168 - val_loss: 1840.6530 - lr: 0.0206\n",
            "\n",
            "Epoch 152: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 152/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 541.9842 - val_loss: 1811.5051 - lr: 0.0206\n",
            "\n",
            "Epoch 153: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 153/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 542.4695 - val_loss: 1822.7031 - lr: 0.0206\n",
            "\n",
            "Epoch 154: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 154/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 535.9520 - val_loss: 1806.8256 - lr: 0.0206\n",
            "\n",
            "Epoch 155: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 155/300\n",
            "416/416 [==============================] - 0s 877us/step - loss: 546.0449 - val_loss: 1798.1719 - lr: 0.0206\n",
            "\n",
            "Epoch 156: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 156/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 541.3099 - val_loss: 1867.6398 - lr: 0.0206\n",
            "\n",
            "Epoch 157: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 157/300\n",
            "416/416 [==============================] - 0s 892us/step - loss: 541.8517 - val_loss: 1836.9185 - lr: 0.0206\n",
            "\n",
            "Epoch 158: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 158/300\n",
            "416/416 [==============================] - 0s 935us/step - loss: 537.2388 - val_loss: 1907.8241 - lr: 0.0206\n",
            "\n",
            "Epoch 159: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 159/300\n",
            "416/416 [==============================] - 0s 880us/step - loss: 535.9857 - val_loss: 1904.5709 - lr: 0.0206\n",
            "\n",
            "Epoch 160: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 160/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 538.8218 - val_loss: 1866.9636 - lr: 0.0206\n",
            "\n",
            "Epoch 161: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 161/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 531.3729 - val_loss: 1802.5494 - lr: 0.0185\n",
            "\n",
            "Epoch 162: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 162/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 532.5876 - val_loss: 1910.4836 - lr: 0.0185\n",
            "\n",
            "Epoch 163: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 163/300\n",
            "416/416 [==============================] - 0s 897us/step - loss: 530.7812 - val_loss: 1800.7563 - lr: 0.0185\n",
            "\n",
            "Epoch 164: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 164/300\n",
            "416/416 [==============================] - 0s 865us/step - loss: 527.6959 - val_loss: 1799.6470 - lr: 0.0185\n",
            "\n",
            "Epoch 165: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 165/300\n",
            "416/416 [==============================] - 0s 868us/step - loss: 522.5630 - val_loss: 1825.1018 - lr: 0.0185\n",
            "\n",
            "Epoch 166: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 166/300\n",
            "416/416 [==============================] - 0s 872us/step - loss: 530.9642 - val_loss: 2123.0154 - lr: 0.0185\n",
            "\n",
            "Epoch 167: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 167/300\n",
            "416/416 [==============================] - 0s 887us/step - loss: 536.4169 - val_loss: 1780.7648 - lr: 0.0185\n",
            "\n",
            "Epoch 168: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 168/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 545.2779 - val_loss: 1823.9115 - lr: 0.0185\n",
            "\n",
            "Epoch 169: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 169/300\n",
            "416/416 [==============================] - 0s 824us/step - loss: 540.2077 - val_loss: 1918.6265 - lr: 0.0185\n",
            "\n",
            "Epoch 170: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 170/300\n",
            "416/416 [==============================] - 0s 856us/step - loss: 532.3561 - val_loss: 1896.2563 - lr: 0.0185\n",
            "\n",
            "Epoch 171: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 171/300\n",
            "416/416 [==============================] - 0s 877us/step - loss: 530.6468 - val_loss: 1814.8300 - lr: 0.0167\n",
            "\n",
            "Epoch 172: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 172/300\n",
            "416/416 [==============================] - 0s 860us/step - loss: 518.4269 - val_loss: 1881.3574 - lr: 0.0167\n",
            "\n",
            "Epoch 173: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 173/300\n",
            "416/416 [==============================] - 0s 865us/step - loss: 528.2083 - val_loss: 1875.6719 - lr: 0.0167\n",
            "\n",
            "Epoch 174: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 174/300\n",
            "416/416 [==============================] - 0s 863us/step - loss: 521.1729 - val_loss: 1841.3362 - lr: 0.0167\n",
            "\n",
            "Epoch 175: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 175/300\n",
            "416/416 [==============================] - 0s 909us/step - loss: 523.5573 - val_loss: 1835.1633 - lr: 0.0167\n",
            "\n",
            "Epoch 176: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 176/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 518.0809 - val_loss: 1948.7489 - lr: 0.0167\n",
            "\n",
            "Epoch 177: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 177/300\n",
            "416/416 [==============================] - 0s 863us/step - loss: 524.7910 - val_loss: 1820.5298 - lr: 0.0167\n",
            "\n",
            "Epoch 178: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 178/300\n",
            "416/416 [==============================] - 0s 856us/step - loss: 519.7469 - val_loss: 1892.1923 - lr: 0.0167\n",
            "\n",
            "Epoch 179: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 179/300\n",
            "416/416 [==============================] - 0s 863us/step - loss: 520.1168 - val_loss: 1805.4905 - lr: 0.0167\n",
            "\n",
            "Epoch 180: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 180/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 524.7526 - val_loss: 1795.3943 - lr: 0.0167\n",
            "\n",
            "Epoch 181: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 181/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 515.6201 - val_loss: 1807.0399 - lr: 0.0150\n",
            "\n",
            "Epoch 182: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 182/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 523.3594 - val_loss: 1803.2233 - lr: 0.0150\n",
            "\n",
            "Epoch 183: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 183/300\n",
            "416/416 [==============================] - 0s 863us/step - loss: 522.8619 - val_loss: 1820.4509 - lr: 0.0150\n",
            "\n",
            "Epoch 184: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 184/300\n",
            "416/416 [==============================] - 0s 875us/step - loss: 517.8716 - val_loss: 1798.3977 - lr: 0.0150\n",
            "\n",
            "Epoch 185: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 185/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 516.8848 - val_loss: 1793.4974 - lr: 0.0150\n",
            "\n",
            "Epoch 186: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 186/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 502.0198 - val_loss: 1806.4119 - lr: 0.0150\n",
            "\n",
            "Epoch 187: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 187/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 511.2543 - val_loss: 1817.8589 - lr: 0.0150\n",
            "\n",
            "Epoch 188: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 188/300\n",
            "416/416 [==============================] - 0s 868us/step - loss: 515.8044 - val_loss: 1828.8127 - lr: 0.0150\n",
            "\n",
            "Epoch 189: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 189/300\n",
            "416/416 [==============================] - 0s 911us/step - loss: 512.6437 - val_loss: 1806.5416 - lr: 0.0150\n",
            "\n",
            "Epoch 190: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 190/300\n",
            "416/416 [==============================] - 0s 880us/step - loss: 515.9555 - val_loss: 1798.5458 - lr: 0.0150\n",
            "\n",
            "Epoch 191: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 191/300\n",
            "416/416 [==============================] - 0s 868us/step - loss: 518.2494 - val_loss: 1797.3097 - lr: 0.0135\n",
            "\n",
            "Epoch 192: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 192/300\n",
            "416/416 [==============================] - 0s 870us/step - loss: 513.4841 - val_loss: 1803.8556 - lr: 0.0135\n",
            "\n",
            "Epoch 193: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 193/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 505.3321 - val_loss: 1828.8906 - lr: 0.0135\n",
            "\n",
            "Epoch 194: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 194/300\n",
            "416/416 [==============================] - 0s 860us/step - loss: 512.0188 - val_loss: 1839.9366 - lr: 0.0135\n",
            "\n",
            "Epoch 195: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 195/300\n",
            "416/416 [==============================] - 0s 870us/step - loss: 507.2286 - val_loss: 1924.2216 - lr: 0.0135\n",
            "\n",
            "Epoch 196: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 196/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 507.9975 - val_loss: 1795.1300 - lr: 0.0135\n",
            "\n",
            "Epoch 197: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 197/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 512.1313 - val_loss: 1808.3685 - lr: 0.0135\n",
            "\n",
            "Epoch 198: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 198/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 502.7119 - val_loss: 1801.7179 - lr: 0.0135\n",
            "\n",
            "Epoch 199: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 199/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 512.1827 - val_loss: 1812.7585 - lr: 0.0135\n",
            "\n",
            "Epoch 200: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 200/300\n",
            "416/416 [==============================] - 0s 824us/step - loss: 499.3449 - val_loss: 1786.5197 - lr: 0.0135\n",
            "\n",
            "Epoch 201: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 201/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 496.2827 - val_loss: 1827.1283 - lr: 0.0122\n",
            "\n",
            "Epoch 202: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 202/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 510.3770 - val_loss: 1807.3938 - lr: 0.0122\n",
            "\n",
            "Epoch 203: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 203/300\n",
            "416/416 [==============================] - 0s 815us/step - loss: 497.2985 - val_loss: 1804.1915 - lr: 0.0122\n",
            "\n",
            "Epoch 204: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 204/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 505.7795 - val_loss: 1837.7792 - lr: 0.0122\n",
            "\n",
            "Epoch 205: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 205/300\n",
            "416/416 [==============================] - 0s 803us/step - loss: 513.9781 - val_loss: 1813.0566 - lr: 0.0122\n",
            "\n",
            "Epoch 206: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 206/300\n",
            "416/416 [==============================] - 0s 812us/step - loss: 500.1179 - val_loss: 1794.7246 - lr: 0.0122\n",
            "\n",
            "Epoch 207: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 207/300\n",
            "416/416 [==============================] - 0s 812us/step - loss: 503.3503 - val_loss: 1809.7168 - lr: 0.0122\n",
            "\n",
            "Epoch 208: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 208/300\n",
            "416/416 [==============================] - 0s 793us/step - loss: 502.8703 - val_loss: 1942.2501 - lr: 0.0122\n",
            "\n",
            "Epoch 209: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 209/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 497.1003 - val_loss: 1830.5718 - lr: 0.0122\n",
            "\n",
            "Epoch 210: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 210/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 500.1704 - val_loss: 1836.6908 - lr: 0.0122\n",
            "\n",
            "Epoch 211: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 211/300\n",
            "416/416 [==============================] - 0s 812us/step - loss: 494.6714 - val_loss: 1830.1707 - lr: 0.0109\n",
            "\n",
            "Epoch 212: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 212/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 493.6512 - val_loss: 1826.0958 - lr: 0.0109\n",
            "\n",
            "Epoch 213: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 213/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 494.2635 - val_loss: 1803.1802 - lr: 0.0109\n",
            "\n",
            "Epoch 214: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 214/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 495.4508 - val_loss: 1910.0566 - lr: 0.0109\n",
            "\n",
            "Epoch 215: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 215/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 495.6329 - val_loss: 1800.0541 - lr: 0.0109\n",
            "\n",
            "Epoch 216: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 216/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 498.0648 - val_loss: 1848.2896 - lr: 0.0109\n",
            "\n",
            "Epoch 217: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 217/300\n",
            "416/416 [==============================] - 0s 803us/step - loss: 497.0722 - val_loss: 1879.8116 - lr: 0.0109\n",
            "\n",
            "Epoch 218: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 218/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 491.5346 - val_loss: 1815.3302 - lr: 0.0109\n",
            "\n",
            "Epoch 219: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 219/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 496.5840 - val_loss: 1811.9043 - lr: 0.0109\n",
            "\n",
            "Epoch 220: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 220/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 501.0872 - val_loss: 1819.0548 - lr: 0.0109\n",
            "\n",
            "Epoch 221: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 221/300\n",
            "416/416 [==============================] - 0s 819us/step - loss: 493.6766 - val_loss: 1805.7970 - lr: 0.0098\n",
            "\n",
            "Epoch 222: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 222/300\n",
            "416/416 [==============================] - 0s 812us/step - loss: 493.3006 - val_loss: 1817.6627 - lr: 0.0098\n",
            "\n",
            "Epoch 223: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 223/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 492.0358 - val_loss: 1844.8997 - lr: 0.0098\n",
            "\n",
            "Epoch 224: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 224/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 491.6111 - val_loss: 1899.5227 - lr: 0.0098\n",
            "\n",
            "Epoch 225: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 225/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 488.8591 - val_loss: 1828.2864 - lr: 0.0098\n",
            "\n",
            "Epoch 226: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 226/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 491.2670 - val_loss: 1808.9459 - lr: 0.0098\n",
            "\n",
            "Epoch 227: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 227/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 491.3231 - val_loss: 1805.0422 - lr: 0.0098\n",
            "\n",
            "Epoch 228: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 228/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 494.8928 - val_loss: 1837.1178 - lr: 0.0098\n",
            "\n",
            "Epoch 229: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 229/300\n",
            "416/416 [==============================] - 0s 803us/step - loss: 493.4616 - val_loss: 1837.3008 - lr: 0.0098\n",
            "\n",
            "Epoch 230: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 230/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 487.9042 - val_loss: 1801.9791 - lr: 0.0098\n",
            "\n",
            "Epoch 231: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 231/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 484.9636 - val_loss: 1810.6537 - lr: 0.0089\n",
            "\n",
            "Epoch 232: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 232/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 486.9910 - val_loss: 1849.0195 - lr: 0.0089\n",
            "\n",
            "Epoch 233: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 233/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 490.4357 - val_loss: 1835.7566 - lr: 0.0089\n",
            "\n",
            "Epoch 234: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 234/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 485.5927 - val_loss: 1844.4830 - lr: 0.0089\n",
            "\n",
            "Epoch 235: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 235/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 489.8028 - val_loss: 1929.4552 - lr: 0.0089\n",
            "\n",
            "Epoch 236: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 236/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 485.4810 - val_loss: 1794.1493 - lr: 0.0089\n",
            "\n",
            "Epoch 237: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 237/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 483.0917 - val_loss: 1874.5308 - lr: 0.0089\n",
            "\n",
            "Epoch 238: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 238/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 486.5160 - val_loss: 1820.9243 - lr: 0.0089\n",
            "\n",
            "Epoch 239: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 239/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 488.8623 - val_loss: 1883.1444 - lr: 0.0089\n",
            "\n",
            "Epoch 240: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 240/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 487.4244 - val_loss: 1811.6226 - lr: 0.0089\n",
            "\n",
            "Epoch 241: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 241/300\n",
            "416/416 [==============================] - 0s 824us/step - loss: 481.2727 - val_loss: 1853.1632 - lr: 0.0080\n",
            "\n",
            "Epoch 242: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 242/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 481.8598 - val_loss: 1815.7974 - lr: 0.0080\n",
            "\n",
            "Epoch 243: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 243/300\n",
            "416/416 [==============================] - 0s 815us/step - loss: 486.5675 - val_loss: 1842.4923 - lr: 0.0080\n",
            "\n",
            "Epoch 244: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 244/300\n",
            "416/416 [==============================] - 0s 819us/step - loss: 484.7626 - val_loss: 1837.3947 - lr: 0.0080\n",
            "\n",
            "Epoch 245: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 245/300\n",
            "416/416 [==============================] - 0s 812us/step - loss: 481.7400 - val_loss: 1847.5366 - lr: 0.0080\n",
            "\n",
            "Epoch 246: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 246/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 484.6844 - val_loss: 1892.5197 - lr: 0.0080\n",
            "\n",
            "Epoch 247: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 247/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 483.0772 - val_loss: 1903.0558 - lr: 0.0080\n",
            "\n",
            "Epoch 248: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 248/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 484.4881 - val_loss: 1810.2794 - lr: 0.0080\n",
            "\n",
            "Epoch 249: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 249/300\n",
            "416/416 [==============================] - 0s 824us/step - loss: 484.0944 - val_loss: 1837.4642 - lr: 0.0080\n",
            "\n",
            "Epoch 250: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 250/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 491.2973 - val_loss: 1817.5372 - lr: 0.0080\n",
            "\n",
            "Epoch 251: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 251/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 477.4288 - val_loss: 1890.8451 - lr: 0.0072\n",
            "\n",
            "Epoch 252: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 252/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 478.1385 - val_loss: 1838.3689 - lr: 0.0072\n",
            "\n",
            "Epoch 253: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 253/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 478.4125 - val_loss: 1869.6172 - lr: 0.0072\n",
            "\n",
            "Epoch 254: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 254/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 480.2063 - val_loss: 1844.3431 - lr: 0.0072\n",
            "\n",
            "Epoch 255: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 255/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 478.1259 - val_loss: 1803.6505 - lr: 0.0072\n",
            "\n",
            "Epoch 256: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 256/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 478.4369 - val_loss: 1813.7013 - lr: 0.0072\n",
            "\n",
            "Epoch 257: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 257/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 479.6913 - val_loss: 1857.3345 - lr: 0.0072\n",
            "\n",
            "Epoch 258: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 258/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 483.4842 - val_loss: 1890.8624 - lr: 0.0072\n",
            "\n",
            "Epoch 259: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 259/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 479.5391 - val_loss: 1805.3744 - lr: 0.0072\n",
            "\n",
            "Epoch 260: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 260/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 480.7162 - val_loss: 1825.0198 - lr: 0.0072\n",
            "\n",
            "Epoch 261: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 261/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 475.1935 - val_loss: 1820.6107 - lr: 0.0065\n",
            "\n",
            "Epoch 262: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 262/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 474.7632 - val_loss: 1834.7648 - lr: 0.0065\n",
            "\n",
            "Epoch 263: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 263/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 473.1582 - val_loss: 1822.9805 - lr: 0.0065\n",
            "\n",
            "Epoch 264: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 264/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 474.4106 - val_loss: 1859.7969 - lr: 0.0065\n",
            "\n",
            "Epoch 265: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 265/300\n",
            "416/416 [==============================] - 0s 824us/step - loss: 474.6454 - val_loss: 1837.6487 - lr: 0.0065\n",
            "\n",
            "Epoch 266: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 266/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 474.8902 - val_loss: 1863.5460 - lr: 0.0065\n",
            "\n",
            "Epoch 267: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 267/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 477.8258 - val_loss: 1815.0594 - lr: 0.0065\n",
            "\n",
            "Epoch 268: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 268/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 477.7563 - val_loss: 1873.8126 - lr: 0.0065\n",
            "\n",
            "Epoch 269: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 269/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 480.1500 - val_loss: 1822.4042 - lr: 0.0065\n",
            "\n",
            "Epoch 270: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 270/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 479.5461 - val_loss: 1846.7863 - lr: 0.0065\n",
            "\n",
            "Epoch 271: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 271/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 469.2263 - val_loss: 1847.4983 - lr: 0.0058\n",
            "\n",
            "Epoch 272: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 272/300\n",
            "416/416 [==============================] - 0s 812us/step - loss: 474.5421 - val_loss: 1821.2040 - lr: 0.0058\n",
            "\n",
            "Epoch 273: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 273/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 471.3094 - val_loss: 1806.2994 - lr: 0.0058\n",
            "\n",
            "Epoch 274: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 274/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 472.3102 - val_loss: 1916.1420 - lr: 0.0058\n",
            "\n",
            "Epoch 275: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 275/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 473.5258 - val_loss: 1886.4622 - lr: 0.0058\n",
            "\n",
            "Epoch 276: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 276/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 475.7737 - val_loss: 1824.1163 - lr: 0.0058\n",
            "\n",
            "Epoch 277: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 277/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 474.3177 - val_loss: 1852.8448 - lr: 0.0058\n",
            "\n",
            "Epoch 278: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 278/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 474.0647 - val_loss: 1822.4500 - lr: 0.0058\n",
            "\n",
            "Epoch 279: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 279/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 472.0714 - val_loss: 1859.5282 - lr: 0.0058\n",
            "\n",
            "Epoch 280: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 280/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 471.8532 - val_loss: 1830.5630 - lr: 0.0058\n",
            "\n",
            "Epoch 281: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 281/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 468.4588 - val_loss: 1824.2159 - lr: 0.0052\n",
            "\n",
            "Epoch 282: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 282/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 469.2533 - val_loss: 1864.4440 - lr: 0.0052\n",
            "\n",
            "Epoch 283: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 283/300\n",
            "416/416 [==============================] - 0s 812us/step - loss: 469.1876 - val_loss: 1825.5812 - lr: 0.0052\n",
            "\n",
            "Epoch 284: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 284/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 471.2329 - val_loss: 1854.3567 - lr: 0.0052\n",
            "\n",
            "Epoch 285: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 285/300\n",
            "416/416 [==============================] - 0s 824us/step - loss: 470.2913 - val_loss: 1931.1516 - lr: 0.0052\n",
            "\n",
            "Epoch 286: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 286/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 467.5921 - val_loss: 1817.5005 - lr: 0.0052\n",
            "\n",
            "Epoch 287: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 287/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 472.2191 - val_loss: 1828.9952 - lr: 0.0052\n",
            "\n",
            "Epoch 288: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 288/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 469.7241 - val_loss: 1828.6155 - lr: 0.0052\n",
            "\n",
            "Epoch 289: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 289/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 469.9643 - val_loss: 1844.0591 - lr: 0.0052\n",
            "\n",
            "Epoch 290: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 290/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 469.9726 - val_loss: 1840.5227 - lr: 0.0052\n",
            "\n",
            "Epoch 291: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 291/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 466.6681 - val_loss: 1849.4617 - lr: 0.0047\n",
            "\n",
            "Epoch 292: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 292/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 466.6755 - val_loss: 1816.1034 - lr: 0.0047\n",
            "\n",
            "Epoch 293: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 293/300\n",
            "416/416 [==============================] - 0s 795us/step - loss: 465.6999 - val_loss: 1852.6407 - lr: 0.0047\n",
            "\n",
            "Epoch 294: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 294/300\n",
            "416/416 [==============================] - 0s 810us/step - loss: 467.0742 - val_loss: 1839.6444 - lr: 0.0047\n",
            "\n",
            "Epoch 295: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 295/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 467.4939 - val_loss: 1833.4280 - lr: 0.0047\n",
            "\n",
            "Epoch 296: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 296/300\n",
            "416/416 [==============================] - 0s 805us/step - loss: 467.9361 - val_loss: 1839.0663 - lr: 0.0047\n",
            "\n",
            "Epoch 297: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 297/300\n",
            "416/416 [==============================] - 0s 800us/step - loss: 466.9138 - val_loss: 1882.3237 - lr: 0.0047\n",
            "\n",
            "Epoch 298: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 298/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 470.0322 - val_loss: 1823.3071 - lr: 0.0047\n",
            "\n",
            "Epoch 299: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 299/300\n",
            "416/416 [==============================] - 0s 798us/step - loss: 469.7486 - val_loss: 1842.3105 - lr: 0.0047\n",
            "\n",
            "Epoch 300: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 300/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 467.7667 - val_loss: 1825.1141 - lr: 0.0047\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import math\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(92, input_dim=92, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    initial_learning_rate = 0.1\n",
        "    decay_rate = 0.9\n",
        "    epoch_rate = 10\n",
        "    return initial_learning_rate * math.pow(decay_rate, math.floor(epoch/epoch_rate))\n",
        "\n",
        "# Compile the model with your desired optimizer and loss function\n",
        "optimizer = Adam(learning_rate=0.1)  # Set initial learning rate\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "# Create a learning rate callback\n",
        "lr_callback = LearningRateScheduler(lr_schedule, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 300, callbacks=[lr_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "416/416 [==============================] - 0s 513us/step - loss: 900.8862\n",
            "104/104 [==============================] - 0s 534us/step - loss: 1864.0299\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the training data\n",
        "train_mse = model.evaluate(X_train, y_train, verbose = 1)\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "test_mse = model.evaluate(X_test, y_test, verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "104/104 [==============================] - 0s 505us/step\n",
            "0.9434772214261157\n"
          ]
        }
      ],
      "source": [
        "# Get predictions for the testing data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Get the r^2\n",
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x238a44a3650>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot loss during training (we can do this because we saved a \"history\" during training)\n",
        "from matplotlib import pyplot\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Loss')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOaeajgRDKbWJPJErwu30Xr",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
