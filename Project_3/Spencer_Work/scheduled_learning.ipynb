{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Caffeine-Jared/CSE450-Team/blob/main/Initial_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w8AJLFDq_-Er"
      },
      "source": [
        "Initial Model - Working through the example code that was provided by the professor."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KSe9ZbMDBIka"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4ggu-08_3mF",
        "outputId": "6993f428-f72e-4c0b-dec5-0519b65e6523"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 16637 entries, 0 to 16636\n",
            "Data columns (total 12 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   dteday        16637 non-null  object \n",
            " 1   season        16637 non-null  int64  \n",
            " 2   hr            16637 non-null  int64  \n",
            " 3   holiday       16637 non-null  int64  \n",
            " 4   workingday    16637 non-null  int64  \n",
            " 5   weathersit    16637 non-null  int64  \n",
            " 6   hum           16637 non-null  float64\n",
            " 7   windspeed     16637 non-null  int64  \n",
            " 8   temp_c        16637 non-null  float64\n",
            " 9   feels_like_c  16637 non-null  float64\n",
            " 10  casual        16637 non-null  int64  \n",
            " 11  registered    16637 non-null  int64  \n",
            "dtypes: float64(3), int64(8), object(1)\n",
            "memory usage: 1.5+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# loading in the bikes csv\n",
        "bikes_df = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')\n",
        "bikes_mini = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/biking_holdout_test_mini.csv')\n",
        "bikes_holdout = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes_december.csv')\n",
        "\n",
        "# check out the info\n",
        "bikes_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>3.28</td>\n",
              "      <td>3.0014</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0</td>\n",
              "      <td>2.34</td>\n",
              "      <td>1.9982</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0</td>\n",
              "      <td>2.34</td>\n",
              "      <td>1.9982</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0</td>\n",
              "      <td>3.28</td>\n",
              "      <td>3.0014</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1/1/11</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0</td>\n",
              "      <td>3.28</td>\n",
              "      <td>3.0014</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dteday  season  hr  holiday  workingday  weathersit   hum  windspeed   \n",
              "0  1/1/11       1   0        0           0           1  0.81          0  \\\n",
              "1  1/1/11       1   1        0           0           1  0.80          0   \n",
              "2  1/1/11       1   2        0           0           1  0.80          0   \n",
              "3  1/1/11       1   3        0           0           1  0.75          0   \n",
              "4  1/1/11       1   4        0           0           1  0.75          0   \n",
              "\n",
              "   temp_c  feels_like_c  casual  registered  \n",
              "0    3.28        3.0014       3          13  \n",
              "1    2.34        1.9982       8          32  \n",
              "2    2.34        1.9982       5          27  \n",
              "3    3.28        3.0014       3          10  \n",
              "4    3.28        3.0014       0           1  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bikes_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ifhWb55CmrN",
        "outputId": "35c9566b-913e-4dff-b500-530554f35f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35 entries, 0 to 34\n",
            "Data columns (total 10 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   dteday        35 non-null     object \n",
            " 1   season        35 non-null     int64  \n",
            " 2   hr            35 non-null     int64  \n",
            " 3   holiday       35 non-null     int64  \n",
            " 4   workingday    35 non-null     int64  \n",
            " 5   weathersit    35 non-null     int64  \n",
            " 6   hum           35 non-null     float64\n",
            " 7   windspeed     35 non-null     int64  \n",
            " 8   temp_c        35 non-null     float64\n",
            " 9   feels_like_c  35 non-null     float64\n",
            "dtypes: float64(3), int64(6), object(1)\n",
            "memory usage: 2.9+ KB\n"
          ]
        }
      ],
      "source": [
        "bikes_mini.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.81</td>\n",
              "      <td>9</td>\n",
              "      <td>4.22</td>\n",
              "      <td>1.9982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>6</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.0014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    dteday  season  hr  holiday  workingday  weathersit   hum  windspeed   \n",
              "0  12/1/12       4   0        0           0           1  0.81          0  \\\n",
              "1  12/1/12       4   1        0           0           1  0.81          0   \n",
              "2  12/1/12       4   2        0           0           2  0.81          0   \n",
              "3  12/1/12       4   3        0           0           2  0.81          9   \n",
              "4  12/1/12       4   4        0           0           1  0.81          6   \n",
              "\n",
              "   temp_c  feels_like_c  \n",
              "0    4.22        3.9980  \n",
              "1    4.22        3.9980  \n",
              "2    4.22        3.9980  \n",
              "3    4.22        1.9982  \n",
              "4    4.22        3.0014  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bikes_mini.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAVPjqQ0Cqqf",
        "outputId": "c18cf7f5-9b43-4fdf-b1f1-09c793bb0ae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 742 entries, 0 to 741\n",
            "Data columns (total 10 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   dteday        742 non-null    object \n",
            " 1   season        742 non-null    int64  \n",
            " 2   hr            742 non-null    int64  \n",
            " 3   holiday       742 non-null    int64  \n",
            " 4   workingday    742 non-null    int64  \n",
            " 5   weathersit    742 non-null    int64  \n",
            " 6   hum           742 non-null    float64\n",
            " 7   windspeed     742 non-null    int64  \n",
            " 8   temp_c        742 non-null    float64\n",
            " 9   feels_like_c  742 non-null    float64\n",
            "dtypes: float64(3), int64(6), object(1)\n",
            "memory usage: 58.1+ KB\n"
          ]
        }
      ],
      "source": [
        "bikes_holdout.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.9980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.81</td>\n",
              "      <td>9</td>\n",
              "      <td>4.22</td>\n",
              "      <td>1.9982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12/1/12</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.81</td>\n",
              "      <td>6</td>\n",
              "      <td>4.22</td>\n",
              "      <td>3.0014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    dteday  season  hr  holiday  workingday  weathersit   hum  windspeed   \n",
              "0  12/1/12       4   0        0           0           1  0.81          0  \\\n",
              "1  12/1/12       4   1        0           0           1  0.81          0   \n",
              "2  12/1/12       4   2        0           0           2  0.81          0   \n",
              "3  12/1/12       4   3        0           0           2  0.81          9   \n",
              "4  12/1/12       4   4        0           0           1  0.81          6   \n",
              "\n",
              "   temp_c  feels_like_c  \n",
              "0    4.22        3.9980  \n",
              "1    4.22        3.9980  \n",
              "2    4.22        3.9980  \n",
              "3    4.22        1.9982  \n",
              "4    4.22        3.0014  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bikes_holdout.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VlXM5n8TGNLM"
      },
      "outputs": [],
      "source": [
        "# Preprocessing \n",
        "bikes_df['dteday'] = pd.to_datetime(bikes_df['dteday'], format='%m/%d/%y')\n",
        "bikes_mini['dteday'] = pd.to_datetime(bikes_mini['dteday'], format='%m/%d/%y')\n",
        "bikes_holdout['dteday'] = pd.to_datetime(bikes_holdout['dteday'], format='%m/%d/%y')\n",
        "\n",
        "# create new features - year, month, day, dayofweek\n",
        "bikes_df['year'] = pd.to_datetime(bikes_df['dteday']).dt.year\n",
        "bikes_df['month'] = pd.to_datetime(bikes_df['dteday']).dt.month\n",
        "bikes_df['day'] = pd.to_datetime(bikes_df['dteday']).dt.day\n",
        "bikes_df['dayofweek'] = pd.to_datetime(bikes_df['dteday']).dt.dayofweek\n",
        "\n",
        "bikes_mini['year'] = pd.to_datetime(bikes_mini['dteday']).dt.year\n",
        "bikes_mini['month'] = pd.to_datetime(bikes_mini['dteday']).dt.month\n",
        "bikes_mini['day'] = pd.to_datetime(bikes_mini['dteday']).dt.day\n",
        "bikes_mini['dayofweek'] = pd.to_datetime(bikes_mini['dteday']).dt.dayofweek\n",
        "\n",
        "bikes_holdout['year'] = pd.to_datetime(bikes_holdout['dteday']).dt.year\n",
        "bikes_holdout['month'] = pd.to_datetime(bikes_holdout['dteday']).dt.month\n",
        "bikes_holdout['day'] = pd.to_datetime(bikes_holdout['dteday']).dt.day\n",
        "bikes_holdout['dayofweek'] = pd.to_datetime(bikes_holdout['dteday']).dt.dayofweek\n",
        "\n",
        "# drop dteday column\n",
        "bikes_df = bikes_df.drop('dteday', axis=1)\n",
        "bikes_mini = bikes_mini.drop('dteday', axis=1)\n",
        "bikes_holdout = bikes_holdout.drop('dteday', axis=1)\n",
        "\n",
        "# one hot encoding\n",
        "categorical_features = ['season', 'hr', 'holiday', 'workingday', 'weathersit', 'year', 'month', 'day', 'dayofweek']\n",
        "bikes_df = pd.get_dummies(bikes_df, columns=categorical_features, dtype=int)\n",
        "bikes_mini = pd.get_dummies(bikes_mini, columns=categorical_features, dtype=int)\n",
        "bikes_holdout = pd.get_dummies(bikes_holdout, columns=categorical_features, dtype=int)\n",
        "\n",
        "# min max scaling\n",
        "scaler = MinMaxScaler()\n",
        "bikes_df[['temp_c', 'hum', 'feels_like_c', 'windspeed']] = scaler.fit_transform(bikes_df[['temp_c', 'hum', 'feels_like_c', 'windspeed']])\n",
        "bikes_mini[['temp_c', 'hum', 'feels_like_c', 'windspeed']] = scaler.fit_transform(bikes_mini[['temp_c', 'hum', 'feels_like_c', 'windspeed']])\n",
        "bikes_holdout[['temp_c', 'hum', 'feels_like_c', 'windspeed']] = scaler.fit_transform(bikes_holdout[['temp_c', 'hum', 'feels_like_c', 'windspeed']])\n",
        "\n",
        "# creating a total count column\n",
        "bikes_df['total_count'] = bikes_df['casual'] + bikes_df['registered']\n",
        "# when it comes to adding these columns together, we don't care about the specifics between casual and registered, we just want the total count, as it provides more information\n",
        "# additionally, the questions we need to answer surround total count, not casual or registered\n",
        "# drop casual and registered columns\n",
        "bikes_df = bikes_df.drop(columns=['casual', 'registered'])\n",
        "# it's important to scale the numbers as not scaling would cause the model to think that the total count is more important than the other features\n",
        "# scale bikes_df total_count\n",
        "#bikes_df[['total_count']] = scaler.fit_transform(bikes_df[['total_count']])\n",
        "# features and the target\n",
        "X = bikes_df.drop(columns=['total_count'])\n",
        "y = bikes_df['total_count']\n",
        "\n",
        "# training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "      <th>season_1</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>season_4</th>\n",
              "      <th>hr_0</th>\n",
              "      <th>hr_1</th>\n",
              "      <th>...</th>\n",
              "      <th>day_29</th>\n",
              "      <th>day_30</th>\n",
              "      <th>day_31</th>\n",
              "      <th>dayofweek_0</th>\n",
              "      <th>dayofweek_1</th>\n",
              "      <th>dayofweek_2</th>\n",
              "      <th>dayofweek_3</th>\n",
              "      <th>dayofweek_4</th>\n",
              "      <th>dayofweek_5</th>\n",
              "      <th>dayofweek_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14616</th>\n",
              "      <td>0.84</td>\n",
              "      <td>0.228070</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.6667</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4605</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.263158</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.6364</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13893</th>\n",
              "      <td>0.84</td>\n",
              "      <td>0.157895</td>\n",
              "      <td>0.693878</td>\n",
              "      <td>0.6667</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2251</th>\n",
              "      <td>0.93</td>\n",
              "      <td>0.298246</td>\n",
              "      <td>0.306122</td>\n",
              "      <td>0.3030</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.192982</td>\n",
              "      <td>0.183673</td>\n",
              "      <td>0.2121</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 92 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        hum  windspeed    temp_c  feels_like_c  season_1  season_2  season_3   \n",
              "14616  0.84   0.228070  0.693878        0.6667         0         0         1  \\\n",
              "4605   0.45   0.263158  0.693878        0.6364         0         0         1   \n",
              "13893  0.84   0.157895  0.693878        0.6667         0         0         1   \n",
              "2251   0.93   0.298246  0.306122        0.3030         0         1         0   \n",
              "1999   0.44   0.192982  0.183673        0.2121         0         1         0   \n",
              "\n",
              "       season_4  hr_0  hr_1  ...  day_29  day_30  day_31  dayofweek_0   \n",
              "14616         0     0     0  ...       0       0       0            0  \\\n",
              "4605          0     0     0  ...       0       0       0            0   \n",
              "13893         0     0     0  ...       0       0       0            0   \n",
              "2251          0     0     0  ...       0       0       0            0   \n",
              "1999          0     0     0  ...       1       0       0            0   \n",
              "\n",
              "       dayofweek_1  dayofweek_2  dayofweek_3  dayofweek_4  dayofweek_5   \n",
              "14616            0            0            1            0            0  \\\n",
              "4605             0            0            0            1            0   \n",
              "13893            1            0            0            0            0   \n",
              "2251             0            0            0            1            0   \n",
              "1999             1            0            0            0            0   \n",
              "\n",
              "       dayofweek_6  \n",
              "14616            0  \n",
              "4605             0  \n",
              "13893            0  \n",
              "2251             0  \n",
              "1999             0  \n",
              "\n",
              "[5 rows x 92 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "      <th>season_1</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>season_4</th>\n",
              "      <th>hr_0</th>\n",
              "      <th>hr_1</th>\n",
              "      <th>...</th>\n",
              "      <th>day_29</th>\n",
              "      <th>day_30</th>\n",
              "      <th>day_31</th>\n",
              "      <th>dayofweek_0</th>\n",
              "      <th>dayofweek_1</th>\n",
              "      <th>dayofweek_2</th>\n",
              "      <th>dayofweek_3</th>\n",
              "      <th>dayofweek_4</th>\n",
              "      <th>dayofweek_5</th>\n",
              "      <th>dayofweek_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.081633</td>\n",
              "      <td>0.0909</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10235</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.491228</td>\n",
              "      <td>0.489796</td>\n",
              "      <td>0.4848</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12309</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.530612</td>\n",
              "      <td>0.5152</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8158</th>\n",
              "      <td>0.28</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.3333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12027</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.298246</td>\n",
              "      <td>0.551020</td>\n",
              "      <td>0.5303</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 92 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        hum  windspeed    temp_c  feels_like_c  season_1  season_2  season_3   \n",
              "191    0.49   0.333333  0.081633        0.0909         1         0         0  \\\n",
              "10235  0.34   0.491228  0.489796        0.4848         1         0         0   \n",
              "12309  0.88   0.000000  0.530612        0.5152         0         1         0   \n",
              "8158   0.28   0.000000  0.285714        0.3333         0         0         0   \n",
              "12027  0.88   0.298246  0.551020        0.5303         0         1         0   \n",
              "\n",
              "       season_4  hr_0  hr_1  ...  day_29  day_30  day_31  dayofweek_0   \n",
              "191           0     0     0  ...       0       0       0            0  \\\n",
              "10235         0     0     0  ...       0       0       0            0   \n",
              "12309         0     0     0  ...       0       0       0            0   \n",
              "8158          1     0     0  ...       0       0       0            0   \n",
              "12027         0     0     0  ...       0       0       0            1   \n",
              "\n",
              "       dayofweek_1  dayofweek_2  dayofweek_3  dayofweek_4  dayofweek_5   \n",
              "191              0            0            0            0            0  \\\n",
              "10235            0            1            0            0            0   \n",
              "12309            0            0            0            0            1   \n",
              "8158             0            0            0            0            0   \n",
              "12027            0            0            0            0            0   \n",
              "\n",
              "       dayofweek_6  \n",
              "191              1  \n",
              "10235            0  \n",
              "12309            0  \n",
              "8158             1  \n",
              "12027            0  \n",
              "\n",
              "[5 rows x 92 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>temp_c</th>\n",
              "      <th>feels_like_c</th>\n",
              "      <th>season_1</th>\n",
              "      <th>season_2</th>\n",
              "      <th>season_3</th>\n",
              "      <th>season_4</th>\n",
              "      <th>hr_0</th>\n",
              "      <th>hr_1</th>\n",
              "      <th>...</th>\n",
              "      <th>day_30</th>\n",
              "      <th>day_31</th>\n",
              "      <th>dayofweek_0</th>\n",
              "      <th>dayofweek_1</th>\n",
              "      <th>dayofweek_2</th>\n",
              "      <th>dayofweek_3</th>\n",
              "      <th>dayofweek_4</th>\n",
              "      <th>dayofweek_5</th>\n",
              "      <th>dayofweek_6</th>\n",
              "      <th>total_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.224490</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.204082</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.204082</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.224490</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.224490</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 93 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    hum  windspeed    temp_c  feels_like_c  season_1  season_2  season_3   \n",
              "0  0.81        0.0  0.224490        0.2879         1         0         0  \\\n",
              "1  0.80        0.0  0.204082        0.2727         1         0         0   \n",
              "2  0.80        0.0  0.204082        0.2727         1         0         0   \n",
              "3  0.75        0.0  0.224490        0.2879         1         0         0   \n",
              "4  0.75        0.0  0.224490        0.2879         1         0         0   \n",
              "\n",
              "   season_4  hr_0  hr_1  ...  day_30  day_31  dayofweek_0  dayofweek_1   \n",
              "0         0     1     0  ...       0       0            0            0  \\\n",
              "1         0     0     1  ...       0       0            0            0   \n",
              "2         0     0     0  ...       0       0            0            0   \n",
              "3         0     0     0  ...       0       0            0            0   \n",
              "4         0     0     0  ...       0       0            0            0   \n",
              "\n",
              "   dayofweek_2  dayofweek_3  dayofweek_4  dayofweek_5  dayofweek_6   \n",
              "0            0            0            0            1            0  \\\n",
              "1            0            0            0            1            0   \n",
              "2            0            0            0            1            0   \n",
              "3            0            0            0            1            0   \n",
              "4            0            0            0            1            0   \n",
              "\n",
              "   total_count  \n",
              "0           16  \n",
              "1           40  \n",
              "2           32  \n",
              "3           13  \n",
              "4            1  \n",
              "\n",
              "[5 rows x 93 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bikes_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13309, 92)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "bikes_df.head()\n",
        "bikes_df.to_csv('bikes_output.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 1/300\n",
            "416/416 [==============================] - 1s 1ms/step - loss: 11084.6709 - val_loss: 4768.1768 - lr: 0.1000\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 2/300\n",
            "416/416 [==============================] - 0s 885us/step - loss: 5224.9087 - val_loss: 6089.7822 - lr: 0.1000\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 3/300\n",
            "416/416 [==============================] - 0s 870us/step - loss: 4685.3032 - val_loss: 3912.3752 - lr: 0.1000\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 4/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 4504.8408 - val_loss: 3845.4810 - lr: 0.1000\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 5/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 4275.9951 - val_loss: 3789.7817 - lr: 0.1000\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 6/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 4063.8699 - val_loss: 4111.0713 - lr: 0.1000\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 7/300\n",
            "416/416 [==============================] - 0s 875us/step - loss: 4013.4475 - val_loss: 3660.3638 - lr: 0.1000\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 8/300\n",
            "416/416 [==============================] - 0s 875us/step - loss: 3701.5027 - val_loss: 3798.6851 - lr: 0.1000\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 9/300\n",
            "416/416 [==============================] - 0s 909us/step - loss: 3584.9343 - val_loss: 4579.8721 - lr: 0.1000\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.1.\n",
            "Epoch 10/300\n",
            "416/416 [==============================] - 0s 877us/step - loss: 3809.6343 - val_loss: 3655.5637 - lr: 0.1000\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 11/300\n",
            "416/416 [==============================] - 0s 856us/step - loss: 3585.4453 - val_loss: 3320.8762 - lr: 0.0900\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 12/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 3567.8599 - val_loss: 3444.1887 - lr: 0.0900\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 13/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 3558.2942 - val_loss: 4101.9429 - lr: 0.0900\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 14/300\n",
            "416/416 [==============================] - 0s 856us/step - loss: 3377.3428 - val_loss: 3642.0757 - lr: 0.0900\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 15/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 3490.6877 - val_loss: 3239.7886 - lr: 0.0900\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 16/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 3512.1553 - val_loss: 3701.1299 - lr: 0.0900\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 17/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 3607.5784 - val_loss: 3499.5786 - lr: 0.0900\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 18/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 3312.5732 - val_loss: 3304.4072 - lr: 0.0900\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 19/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 3280.4060 - val_loss: 3385.6233 - lr: 0.0900\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
            "Epoch 20/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 3318.2266 - val_loss: 3318.7263 - lr: 0.0900\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 21/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 3245.0259 - val_loss: 3249.4363 - lr: 0.0810\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 22/300\n",
            "416/416 [==============================] - 0s 875us/step - loss: 3607.9749 - val_loss: 3208.9470 - lr: 0.0810\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 23/300\n",
            "416/416 [==============================] - 0s 856us/step - loss: 3277.4785 - val_loss: 3274.2603 - lr: 0.0810\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 24/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 3252.4849 - val_loss: 3262.4631 - lr: 0.0810\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 25/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 3237.7876 - val_loss: 2954.9741 - lr: 0.0810\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 26/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 3083.8240 - val_loss: 3438.9136 - lr: 0.0810\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 27/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 3252.7778 - val_loss: 3029.4243 - lr: 0.0810\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 28/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 3212.5347 - val_loss: 2903.9575 - lr: 0.0810\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 29/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 3080.1455 - val_loss: 3504.6499 - lr: 0.0810\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
            "Epoch 30/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 3015.1455 - val_loss: 2831.3682 - lr: 0.0810\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 31/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 2990.8528 - val_loss: 2946.6301 - lr: 0.0729\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 32/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 2970.0840 - val_loss: 4961.8271 - lr: 0.0729\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 33/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2949.7195 - val_loss: 2955.6501 - lr: 0.0729\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 34/300\n",
            "416/416 [==============================] - 0s 868us/step - loss: 3032.8616 - val_loss: 2898.5220 - lr: 0.0729\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 35/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2845.3413 - val_loss: 3127.0208 - lr: 0.0729\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 36/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 3027.9985 - val_loss: 2945.8596 - lr: 0.0729\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 37/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2818.8486 - val_loss: 3054.9221 - lr: 0.0729\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 38/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2813.7681 - val_loss: 3352.1194 - lr: 0.0729\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 39/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 2798.2246 - val_loss: 3146.5459 - lr: 0.0729\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 0.0729.\n",
            "Epoch 40/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2852.9192 - val_loss: 2906.8267 - lr: 0.0729\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 41/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2810.1753 - val_loss: 2827.8518 - lr: 0.0656\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 42/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 2894.6172 - val_loss: 2651.0308 - lr: 0.0656\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 43/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2649.7415 - val_loss: 2748.0999 - lr: 0.0656\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 44/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 2651.1809 - val_loss: 2739.3804 - lr: 0.0656\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 45/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2739.9211 - val_loss: 2628.4810 - lr: 0.0656\n",
            "\n",
            "Epoch 46: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 46/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2674.7603 - val_loss: 2851.4023 - lr: 0.0656\n",
            "\n",
            "Epoch 47: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 47/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2663.9375 - val_loss: 2736.1448 - lr: 0.0656\n",
            "\n",
            "Epoch 48: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 48/300\n",
            "416/416 [==============================] - 0s 856us/step - loss: 2664.4343 - val_loss: 2680.0337 - lr: 0.0656\n",
            "\n",
            "Epoch 49: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 49/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2678.4797 - val_loss: 2572.2837 - lr: 0.0656\n",
            "\n",
            "Epoch 50: LearningRateScheduler setting learning rate to 0.06561.\n",
            "Epoch 50/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2633.7527 - val_loss: 2587.7473 - lr: 0.0656\n",
            "\n",
            "Epoch 51: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 51/300\n",
            "416/416 [==============================] - 0s 860us/step - loss: 2588.9751 - val_loss: 2699.9543 - lr: 0.0590\n",
            "\n",
            "Epoch 52: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 52/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 2626.2913 - val_loss: 3111.2539 - lr: 0.0590\n",
            "\n",
            "Epoch 53: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 53/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2658.5901 - val_loss: 2632.5715 - lr: 0.0590\n",
            "\n",
            "Epoch 54: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 54/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 2604.3313 - val_loss: 2646.4717 - lr: 0.0590\n",
            "\n",
            "Epoch 55: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 55/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 2580.9973 - val_loss: 3028.7268 - lr: 0.0590\n",
            "\n",
            "Epoch 56: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 56/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2573.0437 - val_loss: 3037.7588 - lr: 0.0590\n",
            "\n",
            "Epoch 57: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 57/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2762.2322 - val_loss: 3433.3398 - lr: 0.0590\n",
            "\n",
            "Epoch 58: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 58/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2503.0520 - val_loss: 2839.7158 - lr: 0.0590\n",
            "\n",
            "Epoch 59: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 59/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 2598.7139 - val_loss: 2564.7510 - lr: 0.0590\n",
            "\n",
            "Epoch 60: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
            "Epoch 60/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 2483.7449 - val_loss: 3034.8616 - lr: 0.0590\n",
            "\n",
            "Epoch 61: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 61/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2574.2014 - val_loss: 2509.1951 - lr: 0.0531\n",
            "\n",
            "Epoch 62: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 62/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2517.6904 - val_loss: 2689.2190 - lr: 0.0531\n",
            "\n",
            "Epoch 63: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 63/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2451.7039 - val_loss: 2704.2417 - lr: 0.0531\n",
            "\n",
            "Epoch 64: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 64/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 2613.2981 - val_loss: 2828.3655 - lr: 0.0531\n",
            "\n",
            "Epoch 65: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 65/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2556.0691 - val_loss: 2745.0610 - lr: 0.0531\n",
            "\n",
            "Epoch 66: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 66/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 2427.2412 - val_loss: 2977.0862 - lr: 0.0531\n",
            "\n",
            "Epoch 67: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 67/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 2572.4497 - val_loss: 2814.0276 - lr: 0.0531\n",
            "\n",
            "Epoch 68: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 68/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 2560.9480 - val_loss: 2857.3840 - lr: 0.0531\n",
            "\n",
            "Epoch 69: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 69/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2530.7117 - val_loss: 2572.4856 - lr: 0.0531\n",
            "\n",
            "Epoch 70: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
            "Epoch 70/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2518.1584 - val_loss: 2856.3540 - lr: 0.0531\n",
            "\n",
            "Epoch 71: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 71/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 2548.0742 - val_loss: 2584.2283 - lr: 0.0478\n",
            "\n",
            "Epoch 72: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 72/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2528.0850 - val_loss: 2755.2100 - lr: 0.0478\n",
            "\n",
            "Epoch 73: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 73/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 2537.8938 - val_loss: 2635.0793 - lr: 0.0478\n",
            "\n",
            "Epoch 74: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 74/300\n",
            "416/416 [==============================] - 0s 856us/step - loss: 2369.5188 - val_loss: 2666.3689 - lr: 0.0478\n",
            "\n",
            "Epoch 75: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 75/300\n",
            "416/416 [==============================] - 0s 870us/step - loss: 2460.2039 - val_loss: 2715.0720 - lr: 0.0478\n",
            "\n",
            "Epoch 76: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 76/300\n",
            "416/416 [==============================] - 0s 884us/step - loss: 2440.9045 - val_loss: 2543.0073 - lr: 0.0478\n",
            "\n",
            "Epoch 77: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 77/300\n",
            "416/416 [==============================] - 0s 835us/step - loss: 2443.5640 - val_loss: 2747.6470 - lr: 0.0478\n",
            "\n",
            "Epoch 78: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 78/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2463.6206 - val_loss: 2649.3291 - lr: 0.0478\n",
            "\n",
            "Epoch 79: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 79/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 2661.8291 - val_loss: 2676.8733 - lr: 0.0478\n",
            "\n",
            "Epoch 80: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
            "Epoch 80/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 2393.1697 - val_loss: 2460.5234 - lr: 0.0478\n",
            "\n",
            "Epoch 81: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 81/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 2364.5525 - val_loss: 2606.7842 - lr: 0.0430\n",
            "\n",
            "Epoch 82: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 82/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 2431.9084 - val_loss: 2493.9380 - lr: 0.0430\n",
            "\n",
            "Epoch 83: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 83/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2545.6230 - val_loss: 2712.2817 - lr: 0.0430\n",
            "\n",
            "Epoch 84: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 84/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2408.5964 - val_loss: 2525.0745 - lr: 0.0430\n",
            "\n",
            "Epoch 85: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 85/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 2443.5310 - val_loss: 2984.6277 - lr: 0.0430\n",
            "\n",
            "Epoch 86: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 86/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 2367.4534 - val_loss: 2554.0288 - lr: 0.0430\n",
            "\n",
            "Epoch 87: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 87/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2358.6072 - val_loss: 2466.3857 - lr: 0.0430\n",
            "\n",
            "Epoch 88: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 88/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2462.8328 - val_loss: 2487.4001 - lr: 0.0430\n",
            "\n",
            "Epoch 89: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 89/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2436.8242 - val_loss: 2727.7043 - lr: 0.0430\n",
            "\n",
            "Epoch 90: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
            "Epoch 90/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2394.2773 - val_loss: 3264.1060 - lr: 0.0430\n",
            "\n",
            "Epoch 91: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 91/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2380.2761 - val_loss: 2971.2756 - lr: 0.0387\n",
            "\n",
            "Epoch 92: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 92/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 2380.5066 - val_loss: 2383.4067 - lr: 0.0387\n",
            "\n",
            "Epoch 93: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 93/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2539.8508 - val_loss: 2625.1567 - lr: 0.0387\n",
            "\n",
            "Epoch 94: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 94/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2355.7781 - val_loss: 2431.4121 - lr: 0.0387\n",
            "\n",
            "Epoch 95: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 95/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 2314.9587 - val_loss: 3629.3828 - lr: 0.0387\n",
            "\n",
            "Epoch 96: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 96/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2378.9016 - val_loss: 2628.7434 - lr: 0.0387\n",
            "\n",
            "Epoch 97: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 97/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2311.9316 - val_loss: 2900.1514 - lr: 0.0387\n",
            "\n",
            "Epoch 98: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 98/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2380.1047 - val_loss: 2527.2256 - lr: 0.0387\n",
            "\n",
            "Epoch 99: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 99/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2328.7666 - val_loss: 2609.9592 - lr: 0.0387\n",
            "\n",
            "Epoch 100: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
            "Epoch 100/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2362.4294 - val_loss: 2740.4988 - lr: 0.0387\n",
            "\n",
            "Epoch 101: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 101/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2501.1675 - val_loss: 2893.8838 - lr: 0.0349\n",
            "\n",
            "Epoch 102: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 102/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2417.5894 - val_loss: 2644.3293 - lr: 0.0349\n",
            "\n",
            "Epoch 103: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 103/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2426.9846 - val_loss: 2638.2021 - lr: 0.0349\n",
            "\n",
            "Epoch 104: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 104/300\n",
            "416/416 [==============================] - 0s 856us/step - loss: 2342.3752 - val_loss: 2837.1536 - lr: 0.0349\n",
            "\n",
            "Epoch 105: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 105/300\n",
            "416/416 [==============================] - 0s 838us/step - loss: 2338.6826 - val_loss: 2466.1914 - lr: 0.0349\n",
            "\n",
            "Epoch 106: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 106/300\n",
            "416/416 [==============================] - 0s 843us/step - loss: 2383.0542 - val_loss: 2781.5806 - lr: 0.0349\n",
            "\n",
            "Epoch 107: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 107/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 2392.5115 - val_loss: 2662.2212 - lr: 0.0349\n",
            "\n",
            "Epoch 108: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 108/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2356.1033 - val_loss: 2499.1658 - lr: 0.0349\n",
            "\n",
            "Epoch 109: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 109/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2303.1289 - val_loss: 2621.6001 - lr: 0.0349\n",
            "\n",
            "Epoch 110: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
            "Epoch 110/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2333.6040 - val_loss: 3062.1299 - lr: 0.0349\n",
            "\n",
            "Epoch 111: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 111/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2302.7180 - val_loss: 2449.5015 - lr: 0.0314\n",
            "\n",
            "Epoch 112: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 112/300\n",
            "416/416 [==============================] - 0s 820us/step - loss: 2277.0139 - val_loss: 2382.5200 - lr: 0.0314\n",
            "\n",
            "Epoch 113: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 113/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2290.8408 - val_loss: 2509.8169 - lr: 0.0314\n",
            "\n",
            "Epoch 114: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 114/300\n",
            "416/416 [==============================] - 0s 826us/step - loss: 2308.4216 - val_loss: 2438.5994 - lr: 0.0314\n",
            "\n",
            "Epoch 115: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 115/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2287.6028 - val_loss: 2387.4683 - lr: 0.0314\n",
            "\n",
            "Epoch 116: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 116/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2275.7976 - val_loss: 2432.5857 - lr: 0.0314\n",
            "\n",
            "Epoch 117: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 117/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2278.8511 - val_loss: 3123.3489 - lr: 0.0314\n",
            "\n",
            "Epoch 118: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 118/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2381.4336 - val_loss: 2705.6833 - lr: 0.0314\n",
            "\n",
            "Epoch 119: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 119/300\n",
            "416/416 [==============================] - 0s 825us/step - loss: 2303.1238 - val_loss: 2425.7395 - lr: 0.0314\n",
            "\n",
            "Epoch 120: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
            "Epoch 120/300\n",
            "416/416 [==============================] - 0s 838us/step - loss: 2240.6895 - val_loss: 2433.0659 - lr: 0.0314\n",
            "\n",
            "Epoch 121: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 121/300\n",
            "416/416 [==============================] - 0s 843us/step - loss: 2247.0513 - val_loss: 2386.6772 - lr: 0.0282\n",
            "\n",
            "Epoch 122: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 122/300\n",
            "416/416 [==============================] - 0s 838us/step - loss: 2268.7605 - val_loss: 2391.4065 - lr: 0.0282\n",
            "\n",
            "Epoch 123: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 123/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2238.2969 - val_loss: 2329.4927 - lr: 0.0282\n",
            "\n",
            "Epoch 124: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 124/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 2222.9683 - val_loss: 2462.8960 - lr: 0.0282\n",
            "\n",
            "Epoch 125: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 125/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 2260.8342 - val_loss: 2528.4885 - lr: 0.0282\n",
            "\n",
            "Epoch 126: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 126/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 2232.2026 - val_loss: 2459.7092 - lr: 0.0282\n",
            "\n",
            "Epoch 127: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 127/300\n",
            "416/416 [==============================] - 0s 831us/step - loss: 2231.0872 - val_loss: 2342.5669 - lr: 0.0282\n",
            "\n",
            "Epoch 128: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 128/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 2282.6887 - val_loss: 3004.5654 - lr: 0.0282\n",
            "\n",
            "Epoch 129: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 129/300\n",
            "416/416 [==============================] - 0s 831us/step - loss: 2209.1729 - val_loss: 2340.0254 - lr: 0.0282\n",
            "\n",
            "Epoch 130: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
            "Epoch 130/300\n",
            "416/416 [==============================] - 0s 838us/step - loss: 2217.2458 - val_loss: 2394.5747 - lr: 0.0282\n",
            "\n",
            "Epoch 131: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 131/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 2242.3054 - val_loss: 2594.1707 - lr: 0.0254\n",
            "\n",
            "Epoch 132: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 132/300\n",
            "416/416 [==============================] - 0s 831us/step - loss: 2222.4465 - val_loss: 2549.1731 - lr: 0.0254\n",
            "\n",
            "Epoch 133: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 133/300\n",
            "416/416 [==============================] - 0s 892us/step - loss: 2245.5579 - val_loss: 2406.3779 - lr: 0.0254\n",
            "\n",
            "Epoch 134: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 134/300\n",
            "416/416 [==============================] - 0s 843us/step - loss: 2204.0061 - val_loss: 2421.6948 - lr: 0.0254\n",
            "\n",
            "Epoch 135: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 135/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2188.9465 - val_loss: 2482.4966 - lr: 0.0254\n",
            "\n",
            "Epoch 136: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 136/300\n",
            "416/416 [==============================] - 0s 880us/step - loss: 2184.0444 - val_loss: 2471.9521 - lr: 0.0254\n",
            "\n",
            "Epoch 137: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 137/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2231.5120 - val_loss: 2597.8071 - lr: 0.0254\n",
            "\n",
            "Epoch 138: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 138/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 2247.1924 - val_loss: 2568.5103 - lr: 0.0254\n",
            "\n",
            "Epoch 139: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 139/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2268.2070 - val_loss: 2445.6895 - lr: 0.0254\n",
            "\n",
            "Epoch 140: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
            "Epoch 140/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 2245.4800 - val_loss: 2401.7266 - lr: 0.0254\n",
            "\n",
            "Epoch 141: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 141/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2170.7527 - val_loss: 2548.0286 - lr: 0.0229\n",
            "\n",
            "Epoch 142: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 142/300\n",
            "416/416 [==============================] - 0s 855us/step - loss: 2239.5112 - val_loss: 2479.8948 - lr: 0.0229\n",
            "\n",
            "Epoch 143: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 143/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2178.5825 - val_loss: 2391.6912 - lr: 0.0229\n",
            "\n",
            "Epoch 144: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 144/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2183.0908 - val_loss: 2468.7627 - lr: 0.0229\n",
            "\n",
            "Epoch 145: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 145/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 2304.2107 - val_loss: 2482.0896 - lr: 0.0229\n",
            "\n",
            "Epoch 146: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 146/300\n",
            "416/416 [==============================] - 0s 831us/step - loss: 2205.8394 - val_loss: 2387.7932 - lr: 0.0229\n",
            "\n",
            "Epoch 147: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 147/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2253.8701 - val_loss: 2460.5317 - lr: 0.0229\n",
            "\n",
            "Epoch 148: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 148/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2168.8574 - val_loss: 2376.7815 - lr: 0.0229\n",
            "\n",
            "Epoch 149: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 149/300\n",
            "416/416 [==============================] - 0s 819us/step - loss: 2174.6318 - val_loss: 2350.6208 - lr: 0.0229\n",
            "\n",
            "Epoch 150: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
            "Epoch 150/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2147.6479 - val_loss: 2542.8591 - lr: 0.0229\n",
            "\n",
            "Epoch 151: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 151/300\n",
            "416/416 [==============================] - 0s 840us/step - loss: 2173.1599 - val_loss: 2513.6819 - lr: 0.0206\n",
            "\n",
            "Epoch 152: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 152/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2133.9316 - val_loss: 2344.4023 - lr: 0.0206\n",
            "\n",
            "Epoch 153: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 153/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 2159.9788 - val_loss: 2375.0376 - lr: 0.0206\n",
            "\n",
            "Epoch 154: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 154/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 2150.8953 - val_loss: 2473.2764 - lr: 0.0206\n",
            "\n",
            "Epoch 155: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 155/300\n",
            "416/416 [==============================] - 0s 863us/step - loss: 2204.8215 - val_loss: 2398.8196 - lr: 0.0206\n",
            "\n",
            "Epoch 156: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 156/300\n",
            "416/416 [==============================] - 0s 831us/step - loss: 2189.3840 - val_loss: 2389.1472 - lr: 0.0206\n",
            "\n",
            "Epoch 157: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 157/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 2150.8652 - val_loss: 2576.9336 - lr: 0.0206\n",
            "\n",
            "Epoch 158: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 158/300\n",
            "416/416 [==============================] - 0s 838us/step - loss: 2166.4775 - val_loss: 2340.3242 - lr: 0.0206\n",
            "\n",
            "Epoch 159: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 159/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 2152.3821 - val_loss: 2334.7815 - lr: 0.0206\n",
            "\n",
            "Epoch 160: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
            "Epoch 160/300\n",
            "416/416 [==============================] - 0s 849us/step - loss: 2195.1328 - val_loss: 2447.0015 - lr: 0.0206\n",
            "\n",
            "Epoch 161: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 161/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2176.0925 - val_loss: 2366.4500 - lr: 0.0185\n",
            "\n",
            "Epoch 162: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 162/300\n",
            "416/416 [==============================] - 0s 848us/step - loss: 2160.3792 - val_loss: 2484.8401 - lr: 0.0185\n",
            "\n",
            "Epoch 163: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 163/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2204.5076 - val_loss: 2358.6282 - lr: 0.0185\n",
            "\n",
            "Epoch 164: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 164/300\n",
            "416/416 [==============================] - 0s 860us/step - loss: 2180.4900 - val_loss: 2344.4883 - lr: 0.0185\n",
            "\n",
            "Epoch 165: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 165/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 2168.2627 - val_loss: 2415.4326 - lr: 0.0185\n",
            "\n",
            "Epoch 166: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 166/300\n",
            "416/416 [==============================] - 0s 868us/step - loss: 2160.0991 - val_loss: 2389.9036 - lr: 0.0185\n",
            "\n",
            "Epoch 167: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 167/300\n",
            "416/416 [==============================] - 0s 843us/step - loss: 2134.8164 - val_loss: 2512.1938 - lr: 0.0185\n",
            "\n",
            "Epoch 168: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 168/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2138.3892 - val_loss: 2357.6819 - lr: 0.0185\n",
            "\n",
            "Epoch 169: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 169/300\n",
            "416/416 [==============================] - 0s 850us/step - loss: 2146.6707 - val_loss: 2356.1626 - lr: 0.0185\n",
            "\n",
            "Epoch 170: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
            "Epoch 170/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2124.0886 - val_loss: 2383.5171 - lr: 0.0185\n",
            "\n",
            "Epoch 171: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 171/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 2109.5730 - val_loss: 2340.6482 - lr: 0.0167\n",
            "\n",
            "Epoch 172: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 172/300\n",
            "416/416 [==============================] - 0s 892us/step - loss: 2094.1211 - val_loss: 2365.2288 - lr: 0.0167\n",
            "\n",
            "Epoch 173: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 173/300\n",
            "416/416 [==============================] - 0s 920us/step - loss: 2125.7170 - val_loss: 2337.2681 - lr: 0.0167\n",
            "\n",
            "Epoch 174: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 174/300\n",
            "416/416 [==============================] - 0s 899us/step - loss: 2099.2998 - val_loss: 2366.1931 - lr: 0.0167\n",
            "\n",
            "Epoch 175: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 175/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2105.4753 - val_loss: 2996.8628 - lr: 0.0167\n",
            "\n",
            "Epoch 176: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 176/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2179.8323 - val_loss: 2348.4939 - lr: 0.0167\n",
            "\n",
            "Epoch 177: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 177/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 2129.9221 - val_loss: 2318.7036 - lr: 0.0167\n",
            "\n",
            "Epoch 178: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 178/300\n",
            "416/416 [==============================] - 0s 843us/step - loss: 2166.3735 - val_loss: 2332.1543 - lr: 0.0167\n",
            "\n",
            "Epoch 179: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 179/300\n",
            "416/416 [==============================] - 0s 843us/step - loss: 2134.7341 - val_loss: 2325.8560 - lr: 0.0167\n",
            "\n",
            "Epoch 180: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
            "Epoch 180/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 2157.0435 - val_loss: 2345.8030 - lr: 0.0167\n",
            "\n",
            "Epoch 181: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 181/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2113.6526 - val_loss: 2438.1060 - lr: 0.0150\n",
            "\n",
            "Epoch 182: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 182/300\n",
            "416/416 [==============================] - 0s 838us/step - loss: 2116.7922 - val_loss: 2340.1143 - lr: 0.0150\n",
            "\n",
            "Epoch 183: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 183/300\n",
            "416/416 [==============================] - 0s 865us/step - loss: 2109.8982 - val_loss: 2361.8040 - lr: 0.0150\n",
            "\n",
            "Epoch 184: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 184/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2113.2883 - val_loss: 2317.6135 - lr: 0.0150\n",
            "\n",
            "Epoch 185: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 185/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 2168.1628 - val_loss: 2367.5271 - lr: 0.0150\n",
            "\n",
            "Epoch 186: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 186/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 2157.8794 - val_loss: 2382.5234 - lr: 0.0150\n",
            "\n",
            "Epoch 187: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 187/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2125.4375 - val_loss: 2360.8376 - lr: 0.0150\n",
            "\n",
            "Epoch 188: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 188/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2094.0110 - val_loss: 2416.4038 - lr: 0.0150\n",
            "\n",
            "Epoch 189: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 189/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 2109.6238 - val_loss: 2315.0200 - lr: 0.0150\n",
            "\n",
            "Epoch 190: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
            "Epoch 190/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 2101.7908 - val_loss: 2295.6436 - lr: 0.0150\n",
            "\n",
            "Epoch 191: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 191/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2102.9136 - val_loss: 2302.9348 - lr: 0.0135\n",
            "\n",
            "Epoch 192: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 192/300\n",
            "416/416 [==============================] - 0s 856us/step - loss: 2114.0842 - val_loss: 2330.6975 - lr: 0.0135\n",
            "\n",
            "Epoch 193: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 193/300\n",
            "416/416 [==============================] - 0s 865us/step - loss: 2089.0066 - val_loss: 2490.1873 - lr: 0.0135\n",
            "\n",
            "Epoch 194: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 194/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 2083.8066 - val_loss: 2347.7522 - lr: 0.0135\n",
            "\n",
            "Epoch 195: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 195/300\n",
            "416/416 [==============================] - 0s 863us/step - loss: 2126.2261 - val_loss: 2397.1821 - lr: 0.0135\n",
            "\n",
            "Epoch 196: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 196/300\n",
            "416/416 [==============================] - 0s 863us/step - loss: 2082.3125 - val_loss: 2325.0251 - lr: 0.0135\n",
            "\n",
            "Epoch 197: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 197/300\n",
            "416/416 [==============================] - 0s 843us/step - loss: 2090.5569 - val_loss: 2325.3025 - lr: 0.0135\n",
            "\n",
            "Epoch 198: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 198/300\n",
            "416/416 [==============================] - 0s 870us/step - loss: 2073.0894 - val_loss: 2319.8025 - lr: 0.0135\n",
            "\n",
            "Epoch 199: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 199/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2092.4771 - val_loss: 2340.7185 - lr: 0.0135\n",
            "\n",
            "Epoch 200: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
            "Epoch 200/300\n",
            "416/416 [==============================] - 0s 861us/step - loss: 2084.3142 - val_loss: 2340.7146 - lr: 0.0135\n",
            "\n",
            "Epoch 201: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 201/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 2098.3933 - val_loss: 2349.8086 - lr: 0.0122\n",
            "\n",
            "Epoch 202: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 202/300\n",
            "416/416 [==============================] - 0s 889us/step - loss: 2099.6174 - val_loss: 2346.4578 - lr: 0.0122\n",
            "\n",
            "Epoch 203: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 203/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2108.9055 - val_loss: 2380.7925 - lr: 0.0122\n",
            "\n",
            "Epoch 204: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 204/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 2096.9814 - val_loss: 2341.2961 - lr: 0.0122\n",
            "\n",
            "Epoch 205: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 205/300\n",
            "416/416 [==============================] - 0s 819us/step - loss: 2079.1360 - val_loss: 2334.9839 - lr: 0.0122\n",
            "\n",
            "Epoch 206: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 206/300\n",
            "416/416 [==============================] - 0s 862us/step - loss: 2081.7036 - val_loss: 2310.0312 - lr: 0.0122\n",
            "\n",
            "Epoch 207: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 207/300\n",
            "416/416 [==============================] - 0s 854us/step - loss: 2076.5474 - val_loss: 2461.8501 - lr: 0.0122\n",
            "\n",
            "Epoch 208: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 208/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2097.4468 - val_loss: 2320.6064 - lr: 0.0122\n",
            "\n",
            "Epoch 209: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 209/300\n",
            "416/416 [==============================] - 0s 882us/step - loss: 2106.2515 - val_loss: 2437.6311 - lr: 0.0122\n",
            "\n",
            "Epoch 210: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
            "Epoch 210/300\n",
            "416/416 [==============================] - 0s 894us/step - loss: 2090.7429 - val_loss: 2363.2412 - lr: 0.0122\n",
            "\n",
            "Epoch 211: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 211/300\n",
            "416/416 [==============================] - 0s 870us/step - loss: 2085.0669 - val_loss: 2373.1548 - lr: 0.0109\n",
            "\n",
            "Epoch 212: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 212/300\n",
            "416/416 [==============================] - 0s 867us/step - loss: 2079.3005 - val_loss: 2331.2339 - lr: 0.0109\n",
            "\n",
            "Epoch 213: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 213/300\n",
            "416/416 [==============================] - 0s 892us/step - loss: 2059.9199 - val_loss: 2309.6318 - lr: 0.0109\n",
            "\n",
            "Epoch 214: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 214/300\n",
            "416/416 [==============================] - 0s 872us/step - loss: 2084.8838 - val_loss: 2300.5842 - lr: 0.0109\n",
            "\n",
            "Epoch 215: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 215/300\n",
            "416/416 [==============================] - 0s 840us/step - loss: 2082.0947 - val_loss: 2323.5254 - lr: 0.0109\n",
            "\n",
            "Epoch 216: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 216/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 2085.9719 - val_loss: 2383.9629 - lr: 0.0109\n",
            "\n",
            "Epoch 217: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 217/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 2060.0764 - val_loss: 2365.4937 - lr: 0.0109\n",
            "\n",
            "Epoch 218: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 218/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2086.8125 - val_loss: 2359.1335 - lr: 0.0109\n",
            "\n",
            "Epoch 219: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 219/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2088.8550 - val_loss: 2417.6372 - lr: 0.0109\n",
            "\n",
            "Epoch 220: LearningRateScheduler setting learning rate to 0.010941898913151242.\n",
            "Epoch 220/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 2097.5210 - val_loss: 2331.2190 - lr: 0.0109\n",
            "\n",
            "Epoch 221: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 221/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2066.7266 - val_loss: 2340.7144 - lr: 0.0098\n",
            "\n",
            "Epoch 222: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 222/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2077.7971 - val_loss: 2315.4902 - lr: 0.0098\n",
            "\n",
            "Epoch 223: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 223/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2061.3479 - val_loss: 2333.2654 - lr: 0.0098\n",
            "\n",
            "Epoch 224: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 224/300\n",
            "416/416 [==============================] - 0s 867us/step - loss: 2074.2288 - val_loss: 2320.7834 - lr: 0.0098\n",
            "\n",
            "Epoch 225: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 225/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2050.7009 - val_loss: 2344.5686 - lr: 0.0098\n",
            "\n",
            "Epoch 226: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 226/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 2059.4924 - val_loss: 2346.1392 - lr: 0.0098\n",
            "\n",
            "Epoch 227: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 227/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 2051.9929 - val_loss: 2320.7385 - lr: 0.0098\n",
            "\n",
            "Epoch 228: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 228/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2073.5774 - val_loss: 2410.9561 - lr: 0.0098\n",
            "\n",
            "Epoch 229: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 229/300\n",
            "416/416 [==============================] - 0s 837us/step - loss: 2060.5654 - val_loss: 2344.0471 - lr: 0.0098\n",
            "\n",
            "Epoch 230: LearningRateScheduler setting learning rate to 0.00984770902183612.\n",
            "Epoch 230/300\n",
            "416/416 [==============================] - 0s 865us/step - loss: 2062.0288 - val_loss: 2312.6614 - lr: 0.0098\n",
            "\n",
            "Epoch 231: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 231/300\n",
            "416/416 [==============================] - 0s 875us/step - loss: 2046.1934 - val_loss: 2354.0483 - lr: 0.0089\n",
            "\n",
            "Epoch 232: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 232/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 2045.4957 - val_loss: 2304.7153 - lr: 0.0089\n",
            "\n",
            "Epoch 233: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 233/300\n",
            "416/416 [==============================] - 0s 856us/step - loss: 2044.6144 - val_loss: 2293.2007 - lr: 0.0089\n",
            "\n",
            "Epoch 234: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 234/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 2071.6658 - val_loss: 2667.3357 - lr: 0.0089\n",
            "\n",
            "Epoch 235: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 235/300\n",
            "416/416 [==============================] - 0s 894us/step - loss: 2046.9484 - val_loss: 2328.0420 - lr: 0.0089\n",
            "\n",
            "Epoch 236: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 236/300\n",
            "416/416 [==============================] - 0s 868us/step - loss: 2062.2480 - val_loss: 2318.4924 - lr: 0.0089\n",
            "\n",
            "Epoch 237: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 237/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 2052.7366 - val_loss: 2293.9028 - lr: 0.0089\n",
            "\n",
            "Epoch 238: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 238/300\n",
            "416/416 [==============================] - 0s 870us/step - loss: 2035.3451 - val_loss: 2295.9939 - lr: 0.0089\n",
            "\n",
            "Epoch 239: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 239/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2045.7651 - val_loss: 2362.9204 - lr: 0.0089\n",
            "\n",
            "Epoch 240: LearningRateScheduler setting learning rate to 0.008862938119652507.\n",
            "Epoch 240/300\n",
            "416/416 [==============================] - 0s 875us/step - loss: 2038.1982 - val_loss: 2317.7268 - lr: 0.0089\n",
            "\n",
            "Epoch 241: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 241/300\n",
            "416/416 [==============================] - 0s 889us/step - loss: 2042.7069 - val_loss: 2298.8494 - lr: 0.0080\n",
            "\n",
            "Epoch 242: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 242/300\n",
            "416/416 [==============================] - 0s 889us/step - loss: 2056.1309 - val_loss: 2309.1235 - lr: 0.0080\n",
            "\n",
            "Epoch 243: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 243/300\n",
            "416/416 [==============================] - 0s 887us/step - loss: 2054.6450 - val_loss: 2307.8066 - lr: 0.0080\n",
            "\n",
            "Epoch 244: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 244/300\n",
            "416/416 [==============================] - 0s 894us/step - loss: 2057.3403 - val_loss: 2322.5818 - lr: 0.0080\n",
            "\n",
            "Epoch 245: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 245/300\n",
            "416/416 [==============================] - 0s 860us/step - loss: 2050.2717 - val_loss: 2298.1438 - lr: 0.0080\n",
            "\n",
            "Epoch 246: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 246/300\n",
            "416/416 [==============================] - 0s 875us/step - loss: 2034.3409 - val_loss: 2315.3323 - lr: 0.0080\n",
            "\n",
            "Epoch 247: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 247/300\n",
            "416/416 [==============================] - 0s 872us/step - loss: 2039.9659 - val_loss: 2369.4197 - lr: 0.0080\n",
            "\n",
            "Epoch 248: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 248/300\n",
            "416/416 [==============================] - 0s 911us/step - loss: 2039.7325 - val_loss: 2324.2319 - lr: 0.0080\n",
            "\n",
            "Epoch 249: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 249/300\n",
            "416/416 [==============================] - 0s 892us/step - loss: 2027.8308 - val_loss: 2323.5459 - lr: 0.0080\n",
            "\n",
            "Epoch 250: LearningRateScheduler setting learning rate to 0.007976644307687256.\n",
            "Epoch 250/300\n",
            "416/416 [==============================] - 0s 901us/step - loss: 2027.1212 - val_loss: 2388.2715 - lr: 0.0080\n",
            "\n",
            "Epoch 251: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 251/300\n",
            "416/416 [==============================] - 0s 911us/step - loss: 2028.0332 - val_loss: 2323.0745 - lr: 0.0072\n",
            "\n",
            "Epoch 252: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 252/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 2036.4392 - val_loss: 2306.8254 - lr: 0.0072\n",
            "\n",
            "Epoch 253: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 253/300\n",
            "416/416 [==============================] - 0s 834us/step - loss: 2036.9283 - val_loss: 2292.8098 - lr: 0.0072\n",
            "\n",
            "Epoch 254: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 254/300\n",
            "416/416 [==============================] - 0s 868us/step - loss: 2027.4893 - val_loss: 2301.5369 - lr: 0.0072\n",
            "\n",
            "Epoch 255: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 255/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2020.8613 - val_loss: 2310.2817 - lr: 0.0072\n",
            "\n",
            "Epoch 256: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 256/300\n",
            "416/416 [==============================] - 0s 839us/step - loss: 2026.4767 - val_loss: 2312.1868 - lr: 0.0072\n",
            "\n",
            "Epoch 257: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 257/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2018.9836 - val_loss: 2334.2151 - lr: 0.0072\n",
            "\n",
            "Epoch 258: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 258/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2042.4324 - val_loss: 2303.5103 - lr: 0.0072\n",
            "\n",
            "Epoch 259: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 259/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 2027.9772 - val_loss: 2383.1785 - lr: 0.0072\n",
            "\n",
            "Epoch 260: LearningRateScheduler setting learning rate to 0.00717897987691853.\n",
            "Epoch 260/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 2037.6505 - val_loss: 2306.4204 - lr: 0.0072\n",
            "\n",
            "Epoch 261: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 261/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 2028.9169 - val_loss: 2354.0103 - lr: 0.0065\n",
            "\n",
            "Epoch 262: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 262/300\n",
            "416/416 [==============================] - 0s 819us/step - loss: 2016.9790 - val_loss: 2325.4214 - lr: 0.0065\n",
            "\n",
            "Epoch 263: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 263/300\n",
            "416/416 [==============================] - 0s 807us/step - loss: 2021.3353 - val_loss: 2320.0569 - lr: 0.0065\n",
            "\n",
            "Epoch 264: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 264/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2015.0542 - val_loss: 2423.6404 - lr: 0.0065\n",
            "\n",
            "Epoch 265: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 265/300\n",
            "416/416 [==============================] - 0s 817us/step - loss: 2023.8890 - val_loss: 2310.8999 - lr: 0.0065\n",
            "\n",
            "Epoch 266: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 266/300\n",
            "416/416 [==============================] - 0s 827us/step - loss: 2027.5773 - val_loss: 2348.8186 - lr: 0.0065\n",
            "\n",
            "Epoch 267: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 267/300\n",
            "416/416 [==============================] - 0s 819us/step - loss: 2029.8025 - val_loss: 2297.6411 - lr: 0.0065\n",
            "\n",
            "Epoch 268: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 268/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 2021.6880 - val_loss: 2313.3303 - lr: 0.0065\n",
            "\n",
            "Epoch 269: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 269/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 2030.3453 - val_loss: 2316.5227 - lr: 0.0065\n",
            "\n",
            "Epoch 270: LearningRateScheduler setting learning rate to 0.006461081889226678.\n",
            "Epoch 270/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 2015.2573 - val_loss: 2276.7456 - lr: 0.0065\n",
            "\n",
            "Epoch 271: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 271/300\n",
            "416/416 [==============================] - 0s 829us/step - loss: 2006.8268 - val_loss: 2349.5852 - lr: 0.0058\n",
            "\n",
            "Epoch 272: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 272/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 2018.0847 - val_loss: 2340.2495 - lr: 0.0058\n",
            "\n",
            "Epoch 273: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 273/300\n",
            "416/416 [==============================] - 0s 824us/step - loss: 2017.7075 - val_loss: 2287.5569 - lr: 0.0058\n",
            "\n",
            "Epoch 274: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 274/300\n",
            "416/416 [==============================] - 0s 832us/step - loss: 2012.4160 - val_loss: 2288.6592 - lr: 0.0058\n",
            "\n",
            "Epoch 275: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 275/300\n",
            "416/416 [==============================] - 0s 822us/step - loss: 2021.0281 - val_loss: 2299.5085 - lr: 0.0058\n",
            "\n",
            "Epoch 276: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 276/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 2006.6471 - val_loss: 2323.7964 - lr: 0.0058\n",
            "\n",
            "Epoch 277: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 277/300\n",
            "416/416 [==============================] - 0s 815us/step - loss: 2016.5763 - val_loss: 2375.3850 - lr: 0.0058\n",
            "\n",
            "Epoch 278: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 278/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2016.8357 - val_loss: 2304.6138 - lr: 0.0058\n",
            "\n",
            "Epoch 279: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 279/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2027.5536 - val_loss: 2290.7061 - lr: 0.0058\n",
            "\n",
            "Epoch 280: LearningRateScheduler setting learning rate to 0.00581497370030401.\n",
            "Epoch 280/300\n",
            "416/416 [==============================] - 0s 819us/step - loss: 2017.3821 - val_loss: 2281.3594 - lr: 0.0058\n",
            "\n",
            "Epoch 281: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 281/300\n",
            "416/416 [==============================] - 0s 844us/step - loss: 2011.7307 - val_loss: 2367.2510 - lr: 0.0052\n",
            "\n",
            "Epoch 282: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 282/300\n",
            "416/416 [==============================] - 0s 877us/step - loss: 2014.6876 - val_loss: 2335.6506 - lr: 0.0052\n",
            "\n",
            "Epoch 283: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 283/300\n",
            "416/416 [==============================] - 0s 885us/step - loss: 2012.4412 - val_loss: 2295.2087 - lr: 0.0052\n",
            "\n",
            "Epoch 284: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 284/300\n",
            "416/416 [==============================] - 0s 897us/step - loss: 2002.9680 - val_loss: 2313.3184 - lr: 0.0052\n",
            "\n",
            "Epoch 285: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 285/300\n",
            "416/416 [==============================] - 0s 865us/step - loss: 2008.3075 - val_loss: 2323.1189 - lr: 0.0052\n",
            "\n",
            "Epoch 286: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 286/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2012.8979 - val_loss: 2328.8882 - lr: 0.0052\n",
            "\n",
            "Epoch 287: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 287/300\n",
            "416/416 [==============================] - 0s 865us/step - loss: 2015.7819 - val_loss: 2288.7710 - lr: 0.0052\n",
            "\n",
            "Epoch 288: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 288/300\n",
            "416/416 [==============================] - 0s 872us/step - loss: 2010.6243 - val_loss: 2334.8367 - lr: 0.0052\n",
            "\n",
            "Epoch 289: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 289/300\n",
            "416/416 [==============================] - 0s 860us/step - loss: 2023.2319 - val_loss: 2302.5359 - lr: 0.0052\n",
            "\n",
            "Epoch 290: LearningRateScheduler setting learning rate to 0.00523347633027361.\n",
            "Epoch 290/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 2002.9149 - val_loss: 2327.3953 - lr: 0.0052\n",
            "\n",
            "Epoch 291: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 291/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2003.1110 - val_loss: 2342.5349 - lr: 0.0047\n",
            "\n",
            "Epoch 292: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 292/300\n",
            "416/416 [==============================] - 0s 836us/step - loss: 2013.0925 - val_loss: 2373.6826 - lr: 0.0047\n",
            "\n",
            "Epoch 293: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 293/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 2012.4928 - val_loss: 2295.0916 - lr: 0.0047\n",
            "\n",
            "Epoch 294: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 294/300\n",
            "416/416 [==============================] - 0s 863us/step - loss: 2004.1208 - val_loss: 2288.4468 - lr: 0.0047\n",
            "\n",
            "Epoch 295: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 295/300\n",
            "416/416 [==============================] - 0s 851us/step - loss: 1999.2985 - val_loss: 2332.0667 - lr: 0.0047\n",
            "\n",
            "Epoch 296: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 296/300\n",
            "416/416 [==============================] - 0s 853us/step - loss: 2003.5182 - val_loss: 2339.7698 - lr: 0.0047\n",
            "\n",
            "Epoch 297: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 297/300\n",
            "416/416 [==============================] - 0s 846us/step - loss: 2008.5908 - val_loss: 2323.0833 - lr: 0.0047\n",
            "\n",
            "Epoch 298: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 298/300\n",
            "416/416 [==============================] - 0s 841us/step - loss: 2003.7458 - val_loss: 2304.1860 - lr: 0.0047\n",
            "\n",
            "Epoch 299: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 299/300\n",
            "416/416 [==============================] - 0s 858us/step - loss: 2008.8767 - val_loss: 2277.8716 - lr: 0.0047\n",
            "\n",
            "Epoch 300: LearningRateScheduler setting learning rate to 0.004710128697246249.\n",
            "Epoch 300/300\n",
            "416/416 [==============================] - 0s 860us/step - loss: 1992.2711 - val_loss: 2307.2158 - lr: 0.0047\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import math\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(100, input_dim=92, activation='relu'))\n",
        "model.add(Dense(80, activation='relu'))\n",
        "model.add(Dense(40, activation='relu'))\n",
        "model.add(Dense(20, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    initial_learning_rate = 0.1\n",
        "    decay_rate = 0.9\n",
        "    epoch_rate = 10\n",
        "    return initial_learning_rate * math.pow(decay_rate, math.floor(epoch/epoch_rate))\n",
        "\n",
        "# Compile the model with your desired optimizer and loss function\n",
        "optimizer = Adam(learning_rate=0.1)  # Set initial learning rate\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "# Create a learning rate callback\n",
        "lr_callback = LearningRateScheduler(lr_schedule, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 300, callbacks=[lr_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "416/416 [==============================] - 0s 535us/step - loss: 1985.2817\n",
            "104/104 [==============================] - 0s 544us/step - loss: 2307.2158\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the training data\n",
        "train_mse = model.evaluate(X_train, y_train, verbose = 1)\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "test_mse = model.evaluate(X_test, y_test, verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "104/104 [==============================] - 0s 534us/step\n",
            "0.9300385559552686\n"
          ]
        }
      ],
      "source": [
        "# Get predictions for the testing data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Get the r^2\n",
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x238a46f3650>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot loss during training (we can do this because we saved a \"history\" during training)\n",
        "from matplotlib import pyplot\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Loss')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOaeajgRDKbWJPJErwu30Xr",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
