---
title: "Banco Federal de Finan√ßas Report"
subtitle: "Target Marketing Campaign"
author: "Team 4"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
    
---

```{python}
#| label: libraries
#| include: false
import numpy as np 
import pandas as pd
import altair as alt
url_bank = 'https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bank.csv'

bank = pd.read_csv(url_bank)
```


## Background

_A major telemarketing campaign and the results were inadequete to come to a conclusion. We were assigned to impelment a Machine Learning Algorithim to help find the following:_

_1. Find interesting customer segments based on their previous marketing campaign._
_2. Find a way to identify the types of customers most likely to respond favorably to future marketing campaigns._
_3.Proving to a skeptical Senhor Ferreira that a targeted campaign based on data science will significantly outperform a campaign made up of randomly selected customers._

_Since we're operating in the European Union, we're subject to GDPR compliance requirements. The GDPR doesn't apply in this situation, since we're just building a model, not selling data. In order to use this data under GDPR, we'll need to get consent from the customers in the dataset. We are also using only historic data that has been put into an anonymous format so we will be compliant in that department as well._

## Recommendation
_._


## Methodology


We decided that using a supervised approach would be ideal for this job because we are trying to classify our clientele would have interest in subscribing to a term deposit. Supervised approaches are better for getting a finite answer. When we took our data we decided that use eighty percent of the data for training and the remaining data for testing and validation. We chose this distribution due to this being a relatively small dataset and is the standard distribution for supervised learning models. 

Before we even started applying the data to our learning model. We needed to adjust some of our data to make it more usable. We changed how recently people had been contacted to move the people who had not been contacted into separate groups.  We also made it a point to put our numeric data in groups not using them as individuals. 


## Results, Actions, and Limitations

_._


## Tree

## Metrics

```{python}
#| label: Matrix
#| include: false
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import RandomOverSampler
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
from sklearn.metrics import ConfusionMatrixDisplay


clean = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bank.csv')

# Create some new features
def create_new_features(data):
    data['last_contact'] = data['pdays'].apply(lambda x: 1 if x == 999 else 0)
    data['recent_contact'] = data['pdays'].apply(lambda x: 1 if x < 30 else 0)
    data['previous_contact'] = data['previous'].apply(lambda x: 1 if x > 0 else 0)

create_new_features(clean)

# Encode our features and target as needed
features = ['nr.employed', 'age', 'euribor3m', "campaign", 'cons.conf.idx', 'poutcome', 'last_contact', 'recent_contact', 'previous_contact']
X = pd.get_dummies(clean[features], drop_first=True)
y = clean['y']
# Split our data into training and test data
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)

# Use RandomOverSampler for oversampling
ros = RandomOverSampler(random_state=42)
X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)

# Define the parameter grid to search over
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a random forest classifier
clf = RandomForestClassifier(random_state=25, n_jobs=-1)

clf = RandomForestClassifier(n_estimators=50,
                              max_depth=30,
                              min_samples_split=2,
                              min_samples_leaf=1,
                              random_state=25, n_jobs=-1)

# Train the random forest classifier
clf.fit(X_train, y_train)

# Test the random forest classifier
y_pred = clf.predict(X_test)

disp = ConfusionMatrixDisplay.from_estimator(
  clf,
  X_test,
  y_test,
  cmap=plt.cm.Blues,
)

plt.show()
```

## Questions
__Types of customers respond better on certain days than others?__
```{python}
#| label: jobs
#| include: false
filtered_bank2 = bank.loc[(bank['contact'] == 'cellular') | (bank['contact'] == 'telephone')]


success_rate = filtered_bank2.groupby(['contact', 'job'])['y'].value_counts(normalize=True).reset_index(name='success_rate')
success_rate = success_rate.loc[success_rate['y'] == 'yes']

chart3 = alt.Chart(success_rate).mark_bar().encode(
    x=alt.X('job:N', axis=alt.Axis(title='Job Type')),
    y=alt.Y('success_rate:Q', axis=alt.Axis(format='.0%')),
    color=alt.Color('contact:N', legend=None),
    column=alt.Column('contact:N', header=alt.Header(title='Contact Type')),
    tooltip=[alt.Tooltip('success_rate:Q', format='.2%')],
).properties(
    width=200,
    title="Success Rate of Calling Customers by Contact Type and Job"
)
```

```{python}
#| include: include
chart3
```
## Python Notebooks
